{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.24/06\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uproot\n",
    "import os\n",
    "from script import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood = []\n",
    "data_hood = []\n",
    "\n",
    "masses = [130,150,170,200,250,300,350,400,450,500,600,700,800,1000,1200,1500]\n",
    "FEATURES_CAT1 = [\"dimuon_deltar\", \"dimuon_deltaphi\", \"dimuon_deltaeta\", \"met_pt\", \n",
    "             \"deltar_bjet1_dimuon\", \"deltapt_bjet1_dimuon\", \"deltaeta_bjet1_dimuon\", \n",
    "             \"bjet_1_pt\", \"bjet_1_eta\", \"deltaphi_bjet1_dimuon\",\n",
    "             \"ljet_1_pt\", \"ljet_1_eta\", \"bjet_n\", \"ljet_n\"]\n",
    "\n",
    "FEATURES_CAT2 = [\"dimuon_deltar\", \"dimuon_deltaphi\", \"dimuon_deltaeta\", \"met_pt\",\n",
    "            \"ljet_1_pt\", \"ljet_1_eta\", \"ljet_n\"]\n",
    "\n",
    "INTERVALS = [(115, 180),   # 130\n",
    "             (115, 200),   # 150\n",
    "             (120, 220),   # 170\n",
    "             (150, 250),   # 200\n",
    "             (200, 300),   # 250\n",
    "             (225, 375),   # 300\n",
    "             (275, 425),   # 350\n",
    "             (300, 500),   # 400\n",
    "             (350, 550),   # 450\n",
    "             (350, 650),   # 500\n",
    "             (400, 800),   # 600\n",
    "             (500, 900),   # 700\n",
    "             (600, 1000),  # 800\n",
    "             (700, 1800),  # 1000\n",
    "             (700, 1800),  # 1200\n",
    "             (700, 1800)]  # 1500\n",
    "tanbetas = [2,5,10,15,20,25,30,40,50,60]\n",
    "dibosons = ['WWTo2L2Nu','WZTo3LNu','ZZTo2L2Nu','ZZTo4L']\n",
    "singletops = ['s-channel','t-channel_antitop','t-channel_top','tW_antitop','tW_top']\n",
    "bkgttbar_treename = 'background_treeTTbar_UL2016'\n",
    "ttbar_binned = ['Incl_700to1000','Incl_1000toInf']\n",
    "bkgDY_treename = 'background_treeDY_UL2016'\n",
    "DY_binned = ['ZMM_50to120','ZMM_120to200','ZMM_200to400','ZMM_400to800','ZMM_800to1400','ZMM_1400to2300']\n",
    "datas = ['B','C','D','E','F','G','H']\n",
    "\n",
    "for mA in masses:\n",
    "    for tanb in tanbetas:\n",
    "        sgn_treename = 'signal_treeMSSM_bbA_mA' + str(mA) + '_tanb' + str(tanb) + '_UL2016'\n",
    "        sgn_treename2 = 'signal_treeMSSM_bbH_mA' + str(mA) + '_tanb' + str(tanb) + '_UL2016'\n",
    "        sgn_treename3 = 'signal_treeMSSM_ggA_mA' + str(mA) + '_tanb' + str(tanb) + '_UL2016'\n",
    "        sgn_treename4 = 'signal_treeMSSM_ggH_mA' + str(mA) + '_tanb' + str(tanb) + '_UL2016'\n",
    "        hood.append(sgn_treename)\n",
    "        hood.append(sgn_treename2)\n",
    "        hood.append(sgn_treename3)\n",
    "        hood.append(sgn_treename4)\n",
    "\n",
    "for diboson in dibosons:\n",
    "    bkgdiboson_treename = 'background_treediboson_' + diboson + '_UL2016'\n",
    "    hood.append(bkgdiboson_treename)\n",
    "\n",
    "for st in singletops:\n",
    "    bkgst_treename = 'background_treeST_' + st + '_UL2016'\n",
    "    hood.append(bkgst_treename)\n",
    "    \n",
    "hood.append(bkgttbar_treename)\n",
    "hood.append(bkgDY_treename)\n",
    "\n",
    "for ttbar in ttbar_binned:\n",
    "    bkgttbar_binned_treename = 'background_treeTTbar' + ttbar + '_UL2016'\n",
    "    hood.append(bkgttbar_binned_treename)\n",
    "    \n",
    "for dy in DY_binned:\n",
    "    bkgdy_binned_treename = 'background_tree' + dy + '_UL2016'\n",
    "    hood.append(bkgdy_binned_treename)\n",
    "\n",
    "for data in datas:\n",
    "    data_treename = 'background_treeULRun2016' + data\n",
    "    data_hood.append(data_treename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_dataset(features):\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    a = namedtuple('A', ['train_features'])\n",
    "    a.train_features = namedtuple('A', ['shape'])\n",
    "    a.train_features.shape = (len(features),)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pNN_output_signal(df, mass, pth='./weigths/new/'):\n",
    "    \n",
    "    # model building and loading\n",
    "    df_1 = df[df[\"bjet_n\"] > 0]\n",
    "    df_2 = df[df[\"bjet_n\"] == 0]\n",
    "    \n",
    "    out = np.empty((df.shape[0]))\n",
    "    \n",
    "    # CAT 1\n",
    "    data = get_dummy_dataset(features=FEATURES_CAT1)\n",
    "    model = utils.get_compiled_pnn(data)\n",
    "    utils.load_from_checkpoint(model, path= 'new/pnn-balanced-cat_1-case_2')\n",
    "                   \n",
    "    sig = df_1[df_1['type'] == 1.0]\n",
    "    #bkg = df_1[df_1['type'] == 0.0]\n",
    "    \n",
    "    # seleziona su `mass` e `interval`\n",
    "    x = sig[sig['mA'] == mass][FEATURES_CAT1]\n",
    "    #b = bkg[(bkg['dimuon_mass'] > low) & (bkg['dimuon_mass'] < up)][FEATURES_CAT1]\n",
    "    # features and mass\n",
    "    #x = np.concatenate([s.values, b.values], axis=0)\n",
    "    m = mass * np.ones((x.shape[0], 1))\n",
    "    # apply model\n",
    "    y = model.predict({'x': x, 'm': m}, batch_size=1024)\n",
    "        \n",
    "    y_s = y[:len(x)]\n",
    "    #y_b = y[len( s ) : ] \n",
    "             \n",
    "    # store\n",
    "    np.put(out, x.index.values, y_s)\n",
    "    #out[b.index.values, i] = y_b\n",
    "    \n",
    "    # CAT 2\n",
    "    data = get_dummy_dataset(features=FEATURES_CAT2)\n",
    "    model = utils.get_compiled_pnn(data)\n",
    "    utils.load_from_checkpoint(model, path= 'new/pnn-balanced-cat_2-case_2')\n",
    "\n",
    "    sig = df_2[df_2['type'] == 1.0]\n",
    "    #bkg = df_2[df_2['type'] == 0.0]\n",
    "\n",
    "    # seleziona su `mass` e `interval`\n",
    "    x = sig[sig['mA'] == mass][FEATURES_CAT2]\n",
    "    #b = bkg[(bkg['dimuon_mass'] > low) & (bkg['dimuon_mass'] < up)][FEATURES_CAT2]\n",
    "\n",
    "    # features and mass\n",
    "    #x = np.concatenate([s.values, b.values], axis=0)\n",
    "    m = mass * np.ones((x.shape[0], 1))\n",
    "\n",
    "    # apply model\n",
    "    y = model.predict({'x': x, 'm': m}, batch_size=1024)\n",
    "        \n",
    "    y_s = y[:len(x)]\n",
    "    #y_b = y[len( s ) : ] \n",
    "                   \n",
    "    # store\n",
    "    np.put(out, x.index.values, y_s)\n",
    "    #out[b.index.values, i] = y_b\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pNN_output_background(df, pth='./weigths/new/'):\n",
    "    \n",
    "    # model building and loading\n",
    "    df_1 = df[df[\"bjet_n\"] > 0]\n",
    "    df_2 = df[df[\"bjet_n\"] == 0]\n",
    "    \n",
    "    out = np.empty((df.shape[0], len(INTERVALS)))\n",
    "    \n",
    "    # CAT 1\n",
    "    data = get_dummy_dataset(features=FEATURES_CAT1)\n",
    "    model = utils.get_compiled_pnn(data)\n",
    "    utils.load_from_checkpoint(model, path= 'new/pnn-balanced-cat_1-case_2')\n",
    "                   \n",
    "    #sig = df_1[df_1['type'] == 1.0]\n",
    "    bkg = df_1[df_1['type'] == 0.0]\n",
    "    for i, (mass, (low, up)) in enumerate(zip(masses, INTERVALS)):\n",
    "        # seleziona su `mass` e `interval`\n",
    "        #s = sig[sig['mA'] == mass][FEATURES_CAT1]\n",
    "        b = bkg[(bkg['dimuon_mass'] > low) & (bkg['dimuon_mass'] < up)][FEATURES_CAT1]\n",
    "        # features and mass\n",
    "        x = b.values\n",
    "        m = mass * np.ones((x.shape[0], 1))\n",
    "        # apply model\n",
    "        y_b = model.predict({'x': x, 'm': m}, batch_size=1024)\n",
    "                   \n",
    "        # store\n",
    "        #out[s.index.values, i] = y_s\n",
    "        out[b.index.values, i] = np.squeeze(y_b)\n",
    "    \n",
    "    # CAT 2\n",
    "    data = get_dummy_dataset(features=FEATURES_CAT2)\n",
    "    model = utils.get_compiled_pnn(data)\n",
    "    utils.load_from_checkpoint(model, path= 'new/pnn-balanced-cat_2-case_2')\n",
    "\n",
    "    #sig = df_2[df_2['type'] == 1.0]\n",
    "    bkg = df_2[df_2['type'] == 0.0]\n",
    "\n",
    "    for i, (mass, (low, up)) in enumerate(zip(masses, INTERVALS)):\n",
    "        # seleziona su `mass` e `interval`\n",
    "        #s = sig[sig['mA'] == mass][FEATURES_CAT2]\n",
    "        b = bkg[(bkg['dimuon_mass'] > low) & (bkg['dimuon_mass'] < up)][FEATURES_CAT2]\n",
    "\n",
    "        # features and mass\n",
    "        x = b.values\n",
    "        m = mass * np.ones((x.shape[0], 1))\n",
    "\n",
    "        # apply model\n",
    "        y_b = model.predict({'x': x, 'm': m}, batch_size=1024)\n",
    "                   \n",
    "        # store\n",
    "        #out[s.index.values, i] = y_s\n",
    "        out[b.index.values, i] = np.squeeze(y_b)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_root_signal(input_file, tree, array_of_pNN):\n",
    "    print(\"Save new branch in original ROOT file\")\n",
    "    myfile = ROOT.TFile(input_file, 'update')\n",
    "    mytree = myfile.Get(tree)\n",
    "    pNN_output = np.array([0.5])\n",
    "    newBranch = mytree.Branch(\"pNN_output\", pNN_output, \"pNN_output/D\")\n",
    "    numOfEvents = mytree.GetEntries()\n",
    "    for n in range(numOfEvents):\n",
    "        pNN_output[0] = array_of_pNN[n]\n",
    "        mytree.GetEntry(n)\n",
    "        newBranch.Fill()\n",
    "    mytree.Write(\"\", ROOT.TFile.kOverwrite)\n",
    "    myfile.Close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_root_background(input_file, tree, array_of_pNN):\n",
    "    print(\"Save new branch in original ROOT file\")\n",
    "    myfile = ROOT.TFile(input_file, 'update')\n",
    "    mytree = myfile.Get(tree)\n",
    "    listOfNewBranches = []\n",
    "    pNN_output_130 = np.array([0.5])\n",
    "    pNN_output_150 = np.array([0.5])\n",
    "    pNN_output_170 = np.array([0.5])\n",
    "    pNN_output_200 = np.array([0.5])\n",
    "    pNN_output_250 = np.array([0.5])\n",
    "    pNN_output_300 = np.array([0.5])\n",
    "    pNN_output_350 = np.array([0.5])\n",
    "    pNN_output_400 = np.array([0.5])\n",
    "    pNN_output_450 = np.array([0.5])\n",
    "    pNN_output_500 = np.array([0.5])\n",
    "    pNN_output_600 = np.array([0.5])\n",
    "    pNN_output_700 = np.array([0.5])\n",
    "    pNN_output_800 = np.array([0.5])\n",
    "    pNN_output_1000 = np.array([0.5])\n",
    "    pNN_output_1200 = np.array([0.5])\n",
    "    pNN_output_1500 = np.array([0.5])\n",
    "\n",
    "    listOfNewBranches.append(mytree.Branch(\"pNN_output_130\", pNN_output_130, \"pNN_output_130/D\"))\n",
    "    listOfNewBranches.append(mytree.Branch(\"pNN_output_150\", pNN_output_150, \"pNN_output_150/D\"))\n",
    "    listOfNewBranches.append(mytree.Branch(\"pNN_output_170\", pNN_output_170, \"pNN_output_170/D\"))\n",
    "    listOfNewBranches.append(mytree.Branch(\"pNN_output_200\", pNN_output_200, \"pNN_output_200/D\"))\n",
    "    listOfNewBranches.append(mytree.Branch(\"pNN_output_250\", pNN_output_250, \"pNN_output_250/D\"))\n",
    "    listOfNewBranches.append(mytree.Branch(\"pNN_output_300\", pNN_output_300, \"pNN_output_300/D\"))\n",
    "    listOfNewBranches.append(mytree.Branch(\"pNN_output_350\", pNN_output_350, \"pNN_output_350/D\"))\n",
    "    listOfNewBranches.append(mytree.Branch(\"pNN_output_400\", pNN_output_400, \"pNN_output_400/D\"))\n",
    "    listOfNewBranches.append(mytree.Branch(\"pNN_output_450\", pNN_output_450, \"pNN_output_450/D\"))\n",
    "    listOfNewBranches.append(mytree.Branch(\"pNN_output_500\", pNN_output_500, \"pNN_output_500/D\"))\n",
    "    listOfNewBranches.append(mytree.Branch(\"pNN_output_600\", pNN_output_600, \"pNN_output_600/D\"))\n",
    "    listOfNewBranches.append(mytree.Branch(\"pNN_output_700\", pNN_output_700, \"pNN_output_700/D\"))\n",
    "    listOfNewBranches.append(mytree.Branch(\"pNN_output_800\", pNN_output_800, \"pNN_output_800/D\"))\n",
    "    listOfNewBranches.append(mytree.Branch(\"pNN_output_1000\", pNN_output_1000, \"pNN_output_1000/D\"))\n",
    "    listOfNewBranches.append(mytree.Branch(\"pNN_output_1200\", pNN_output_1200, \"pNN_output_1200/D\"))\n",
    "    listOfNewBranches.append(mytree.Branch(\"pNN_output_1500\", pNN_output_1500, \"pNN_output_1500/D\"))\n",
    "            \n",
    "    numOfEvents = mytree.GetEntries()\n",
    "    for n in range(numOfEvents):\n",
    "        pNN_output_130[0] = array_of_pNN[n,0]\n",
    "        pNN_output_150[0] = array_of_pNN[n,1]\n",
    "        pNN_output_170[0] = array_of_pNN[n,2]\n",
    "        pNN_output_200[0] = array_of_pNN[n,3]\n",
    "        pNN_output_250[0] = array_of_pNN[n,4]\n",
    "        pNN_output_300[0] = array_of_pNN[n,5]\n",
    "        pNN_output_350[0] = array_of_pNN[n,6]\n",
    "        pNN_output_400[0] = array_of_pNN[n,7]\n",
    "        pNN_output_450[0] = array_of_pNN[n,8]\n",
    "        pNN_output_500[0] = array_of_pNN[n,9]\n",
    "        pNN_output_600[0] = array_of_pNN[n,10]\n",
    "        pNN_output_700[0] = array_of_pNN[n,11]\n",
    "        pNN_output_800[0] = array_of_pNN[n,12]\n",
    "        pNN_output_1000[0] = array_of_pNN[n,13]\n",
    "        pNN_output_1200[0] = array_of_pNN[n,14]\n",
    "        pNN_output_1500[0] = array_of_pNN[n,15]\n",
    "                \n",
    "        mytree.GetEntry(n)\n",
    "        for newBranch in listOfNewBranches:\n",
    "            newBranch.Fill()\n",
    "            \n",
    "    mytree.Write(\"\", ROOT.TFile.kOverwrite)\n",
    "    myfile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading trees signal_treeMSSM_bbH_mA150_tanb50_UL2016 in file Out_MSSM_bbH_mA150_tanb50_UL2016.root\n",
      "Loaded from \"weights/new/pnn-balanced-cat_1-case_2/weights-04-0.989\"\n",
      "Loaded from \"weights/new/pnn-balanced-cat_2-case_2/weights-73-0.976\"\n",
      "Save new branch in original ROOT file\n",
      "Reading trees background_treediboson_ZZTo2L2Nu_UL2016 in file Out_diboson_ZZTo2L2Nu_UL2016.root\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).sig\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).bkg\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).sig\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).bkg\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Loaded from \"weights/new/pnn-balanced-cat_1-case_2/weights-04-0.989\"\n",
      "Loaded from \"weights/new/pnn-balanced-cat_2-case_2/weights-73-0.976\"\n",
      "Save new branch in original ROOT file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-23 23:03:18.793726: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-23 23:03:18.932541: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "fname = './root_file/'\n",
    "fileIn = fname\n",
    "valid_header=['mA', 'training', 'dimuon_deltar', 'dimuon_deltaphi', 'dimuon_deltaeta', 'dimuon_mass', 'dimuon_pt', 'met_pt', 'met_phi', 'met_eta', 'bjet_n', 'bjet_1_pt', 'bjet_1_eta', 'jetfwd_n', 'ljet_n', 'ljet_1_pt', 'ljet_1_eta', 'deltar_bjet1_dimuon', 'deltapt_bjet1_dimuon', 'deltaeta_bjet1_dimuon', 'deltaphi_bjet1_dimuon', 'PU_Weight']\n",
    "\n",
    "for tree in hood:\n",
    "    for file in os.listdir(fileIn):\n",
    "        if tree.split('_')[0] == 'signal':\n",
    "            if file.split('_')[2] == 'bbH' or file.split('_')[2] == 'bbA':\n",
    "                if file.split('_')[1] + '_' + file.split('_')[2] + '_' + file.split('_')[3] + '_' + file.split('_')[4] != tree.split('_')[1][4:8] + '_' + tree.split('_')[2] + '_' + tree.split('_')[3] + '_' + tree.split('_')[4]:\n",
    "                    continue\n",
    "                print (\"Reading trees {} in file {}\".format(tree, file))\n",
    "                data = uproot.open(fname + file)[tree]\n",
    "                out = data.arrays(valid_header, library=\"pd\")\n",
    "                out[\"type\"] = 1.0\n",
    "                mass = float(file.split('_')[3][2:])\n",
    "                pNN_out_array = get_pNN_output_signal(out, mass)\n",
    "                #update root file with output pNN\n",
    "                rewrite_root_signal(fileIn+file, tree, pNN_out_array)\n",
    "                \n",
    "        if tree.split('_')[0] == 'background':\n",
    "            if file.split('_')[1] == 'diboson':\n",
    "                if file.split('_')[1] + file.split('_')[2] != tree.split('_')[1][4:11] + tree.split('_')[2]:\n",
    "                    continue\n",
    "                print (\"Reading trees {} in file {}\".format(tree, file))\n",
    "                data = uproot.open(fname + file)[tree]\n",
    "                out = data.arrays(valid_header, library=\"pd\")\n",
    "                out[\"type\"] = 0.0\n",
    "                pNN_out_array = get_pNN_output_background(out)\n",
    "                #update root file with output pNN\n",
    "                rewrite_root_background(fileIn+file, tree, pNN_out_array)\n",
    "\n",
    "            elif file.split('_')[1] == 'ST':\n",
    "                if file.split('_')[2:-1] != tree.split('_')[2:-1]:\n",
    "                    continue\n",
    "                print (\"Reading trees {} in file {}\".format(tree, file))\n",
    "                data = uproot.open(fname + file)[tree]\n",
    "                out = data.arrays(valid_header, library=\"pd\")\n",
    "                out[\"type\"] = 0.0\n",
    "                pNN_out_array = get_pNN_output_background(out)\n",
    "                rewrite_root_background(fileIn+file, tree, pNN_out_array)\n",
    "                \n",
    "            elif file.split('_')[1] == 'DY':\n",
    "                if file.split('_')[1] != tree.split('_')[1][4:6]:\n",
    "                    continue\n",
    "                print (\"Reading trees {} in file {}\".format(tree, file))\n",
    "                data = uproot.open(fname + file)[tree]\n",
    "                out = data.arrays(valid_header, library=\"pd\")\n",
    "                out[\"type\"] = 0.0\n",
    "                pNN_out_array = get_pNN_output_background(out)\n",
    "                rewrite_root_background(fileIn+file, tree, pNN_out_array)\n",
    "\n",
    "            elif file.split('_')[1] == 'TTbarIncl':\n",
    "                if file.split('_')[1] != tree.split('_')[1][4:13]:\n",
    "                    continue\n",
    "                if file.split('_')[2] != tree.split('_')[2]:\n",
    "                    continue\n",
    "                print (\"Reading trees {} in file {}\".format(tree, file))\n",
    "                data = uproot.open(fname + file)[tree]\n",
    "                out = data.arrays(valid_header, library=\"pd\")\n",
    "                out[\"type\"] = 0.0\n",
    "                pNN_out_array = get_pNN_output_background(out)\n",
    "                rewrite_root_background(fileIn+file, tree, pNN_out_array)\n",
    "\n",
    "            elif file.split('_')[1] == 'TTbar':\n",
    "                if file.split('_')[1] != tree.split('_')[1][4:9] or tree.split('_')[1][4:13]==\"TTbarIncl\":\n",
    "                    continue\n",
    "                print (\"Reading trees {} in file {}\".format(tree, file))\n",
    "                data = uproot.open(fname + file)[tree]\n",
    "                out = data.arrays(valid_header, library=\"pd\")\n",
    "                out[\"type\"] = 0.0\n",
    "                pNN_out_array = get_pNN_output_background(out)\n",
    "                rewrite_root_background(fileIn+file, tree, pNN_out_array)\n",
    "\n",
    "            elif file.split('_')[1] == 'ZMM':\n",
    "                if file.split('_')[1] != tree.split('_')[1][4:7]:\n",
    "                    continue\n",
    "                if file.split('_')[2] != tree.split('_')[2]:\n",
    "                    continue\n",
    "                print (\"Reading trees {} in file {}\".format(tree, file))\n",
    "                data = uproot.open(fname + file)[tree]\n",
    "                out = data.arrays(valid_header, library=\"pd\")\n",
    "                out[\"type\"] = 0.0\n",
    "                pNN_out_array = get_pNN_output_background(out)\n",
    "                rewrite_root_background(fileIn+file, tree, pNN_out_array)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop for data\n",
    "\n",
    "for tree in data_hood: \n",
    "    for file in os.listdir(fileIn): \n",
    "        if tree.split('_')[0] == 'background': \n",
    "            if file.split('_')[1][2:9] == 'Run2016':     \n",
    "                if file.split('_')[1][2:10] != tree.split('_')[1][6:14]:\n",
    "                    continue\n",
    "                print (\"Reading trees {} in file {}\".format(tree, file))\n",
    "                data = uproot.open(fname + file)[tree]\n",
    "                out = data.arrays(valid_header, library=\"pd\")\n",
    "                out.to_csv(\"./output_csv/\" + file.split(\".\")[0] + \".csv\") \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
