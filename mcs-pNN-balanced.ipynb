{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pNN Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, average_precision_score, roc_auc_score\n",
    "\n",
    "from script import utils\n",
    "from script.utils import free_mem\n",
    "\n",
    "from script.datasets import Dataset, FairDataset\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(df, amount: int, seed):\n",
    "    amount = int(amount)\n",
    "    if amount > df.shape[0]:\n",
    "        x = []\n",
    "        \n",
    "        while amount > 0:\n",
    "            x.append(df.sample(n=min(amount, df.shape[0]), random_state=seed))\n",
    "            amount -= df.shape[0]\n",
    "        \n",
    "        return pd.concat(x, axis=0)\n",
    "    \n",
    "    return df.sample(n=amount, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(dataset: Dataset, valid_size=0.25, test_size=0.2, seed=utils.SEED):\n",
    "    sig = dataset.signal\n",
    "    bkg = dataset.background\n",
    "    \n",
    "    # test split\n",
    "    train_sig, test_sig = train_test_split(sig, test_size=test_size, random_state=seed)\n",
    "    train_bkg, test_bkg = train_test_split(bkg, test_size=test_size, random_state=seed)\n",
    "    \n",
    "    # train-valid split\n",
    "    train_sig, valid_sig = train_test_split(train_sig, test_size=valid_size, random_state=seed)\n",
    "    train_bkg, valid_bkg = train_test_split(train_bkg, test_size=valid_size, random_state=seed)\n",
    "    \n",
    "    return (train_sig, train_bkg), (valid_sig, valid_bkg), (test_sig, test_bkg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, signal: pd.DataFrame, background: pd.DataFrame, batch_size: int, \n",
    "                 features: list, delta=50, balance_signal=True, balance_bkg=True, seed=utils.SEED,\n",
    "                 sample_mass=False):\n",
    "        \"\"\"keras.Sequence that balances the signal (each mA has the same number of events), and backgrounds\"\"\"\n",
    "        self.sig = signal\n",
    "        self.bkg = background\n",
    "        \n",
    "        self.mass = sorted(self.sig['mA'].unique())\n",
    "        self.mass_delta = float(delta)\n",
    "        self.should_sample_mass = bool(sample_mass)\n",
    "        \n",
    "        self.rnd = np.random.RandomState(seed)  # \"slow\" but enough for pd.DataFrame.sample\n",
    "        self.gen = utils.get_random_generator(seed)   # \"fast\" random generator for `np.random.choice`\n",
    "        \n",
    "        self.should_balance_sig = bool(balance_signal)\n",
    "        self.should_balance_bkg = bool(balance_bkg)\n",
    "        \n",
    "        self.features = features\n",
    "        self.half_batch = batch_size // 2\n",
    "        \n",
    "        if self.should_balance_sig:\n",
    "            self.signals = {m: self.sig[self.sig['mA'] == m] for m in self.mass}\n",
    "            \n",
    "            self.sig_batch = self.half_batch // len(self.signals.keys())\n",
    "        else:\n",
    "            self.sig_batch = self.half_batch\n",
    "            \n",
    "        if self.should_balance_bkg:\n",
    "            self.bkgs = {k: self.bkg[self.bkg['name'] == k] for k in self.bkg['name'].unique()}\n",
    "            \n",
    "            self.bkg_batch = self.half_batch // len(self.bkgs.keys())\n",
    "        else:\n",
    "            self.bkg_batch = self.half_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.sig.shape[0] // self.half_batch\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.should_balance_sig:\n",
    "            df = [sample(sig, amount=self.sig_batch, seed=self.rnd) for sig in self.signals.values()]\n",
    "        else:\n",
    "            df = [sample(self.sig, amount=self.half_batch, seed=self.rnd)]\n",
    "        \n",
    "        if self.should_balance_bkg:\n",
    "            df.extend([sample(bkg, amount=self.bkg_batch, seed=self.rnd) for bkg in self.bkgs.values()])\n",
    "        else:\n",
    "            df.append(sample(self.bkg, amount=self.half_batch, seed=self.rnd))\n",
    "        \n",
    "        df = pd.concat(df, axis=0)\n",
    "        \n",
    "        x = df[self.features].values\n",
    "        m = np.reshape(df['mA'].values, newshape=(-1, 1))\n",
    "        y = np.reshape(df['type'].values, newshape=(-1, 1))\n",
    "        \n",
    "        if self.should_sample_mass:\n",
    "            # sample mass (from signal's mA) for background events -> uses all background events\n",
    "            mask = np.squeeze(y == 0.0)\n",
    "            m[mask] = self.gen.choice(self.mass, size=np.sum(mask), replace=True).reshape((-1, 1))\n",
    "        else:\n",
    "            # take mass from corresponding mass interval (m - delta, m + delta)\n",
    "            idx = np.digitize(df['dimuon_M'], data.unique_signal_mass, right=True)\n",
    "            idx = np.clip(idx, a_min=0, a_max=len(data.unique_signal_mass) - 1)\n",
    "        \n",
    "            m = np.array(data.unique_signal_mass)[idx]\n",
    "            m = np.reshape(m, newshape=(-1, 1))\n",
    "        \n",
    "        return dict(x=x, m=m), y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_from_sequence(sequence: tf.keras.utils.Sequence, prefetch=2):\n",
    "    def gen():    \n",
    "        for i in range(len(sequence)):\n",
    "            yield sequence[0]\n",
    "            \n",
    "    tf_data = tf.data.Dataset.from_generator(\n",
    "        gen, \n",
    "        output_types=({'x': tf.float32, 'm': tf.float32}, tf.float32))\n",
    "    \n",
    "    return tf_data.prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset: Dataset, features: list, case: int, train_batch=128, eval_batch=1024, **kwargs):\n",
    "    # split data\n",
    "    train, valid, test = train_val_test_split(dataset, **kwargs)\n",
    "    \n",
    "    # create sequences\n",
    "    train_seq = BalancedSequence(signal=train[0], background=train[1], batch_size=train_batch, \n",
    "                                 features=features, sample_mass=case == 1)\n",
    "\n",
    "    valid_seq = BalancedSequence(signal=valid[0], background=valid[1], batch_size=eval_batch,\n",
    "                                 features=features, balance_signal=False, balance_bkg=False, sample_mass=False)\n",
    "\n",
    "    test_seq = BalancedSequence(signal=test[0], background=test[1], batch_size=eval_batch, \n",
    "                                features=features, balance_signal=False, balance_bkg=False, sample_mass=False)\n",
    "    \n",
    "    # create tf.Datasets\n",
    "    train_ds = dataset_from_sequence(train_seq)\n",
    "    valid_ds = dataset_from_sequence(valid_seq)\n",
    "    test_ds = dataset_from_sequence(test_seq)\n",
    "    \n",
    "    return train_ds, valid_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmsplot(model, dataset, mass: int, category: int, signal: str, delta=50, bins=20, \n",
    "            size=(12, 10), legend='best', title='pNN output distribution', seed=utils.SEED,\n",
    "            path='plot', save=None, show=True, ax=None):\n",
    "    \"\"\"Plots the output distribution of the model, along it's weighted significance\"\"\"\n",
    "    \n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=size)\n",
    "        ax = fig.gca()\n",
    "    \n",
    "    # select both signal and background in interval (m-d, m+d)\n",
    "    sig = dataset.signal[dataset.signal['mA'] == mass]\n",
    "    sig = sig[(sig['dimuon_M'] >= mass - delta) & (sig['dimuon_M'] < mass + delta)]\n",
    "\n",
    "    bkg = dataset.background\n",
    "    bkg = bkg[(bkg['dimuon_M'] >= mass - delta) & (bkg['dimuon_M'] < mass + delta)]\n",
    "    \n",
    "    num_sig = sig.shape[0]\n",
    "    \n",
    "    # prepare data\n",
    "    x = pd.concat([sig[dataset.columns['feature']],\n",
    "                   bkg[dataset.columns['feature']]], axis=0).values\n",
    "\n",
    "    y = np.reshape(pd.concat([sig['type'], bkg['type']], axis=0).values, newshape=[-1])\n",
    "    x = dict(x=x, m=np.ones_like(y[:, np.newaxis]) * mass)\n",
    "    \n",
    "    # predict data\n",
    "    out = model.predict(x=x, batch_size=1024, verbose=0)\n",
    "    out = np.asarray(out)\n",
    "\n",
    "    y_sig = np.squeeze(out[y == 1.0])\n",
    "    y_bkg = np.squeeze(out[y == 0.0])\n",
    "\n",
    "    w_bkg = bkg['weight'].values\n",
    "    w_sig = np.ones_like(y_sig)\n",
    "    \n",
    "    # plot\n",
    "    names = dataset.names_df.loc[bkg.index.values]\n",
    "    df = pd.DataFrame({'Output': y_bkg, 'Bkg': np.squeeze(names), 'weight': w_bkg})\n",
    "    \n",
    "    # plot histograms\n",
    "    sns.histplot(data=df, x='Output', hue='Bkg', multiple='stack', edgecolor='.3', linewidth=0.5, bins=bins,\n",
    "                 weights='weight', ax=ax,\n",
    "                 palette={'DY': 'green', 'TTbar': 'red', 'ST': 'blue', 'diboson': 'yellow'})\n",
    "    \n",
    "    h_bkg, _ = np.histogram(y_bkg, bins=bins, weights=w_bkg)\n",
    "    h_bkg = np.sum(h_bkg)\n",
    "    \n",
    "    h_sig, _ = np.histogram(y_sig, bins=bins)\n",
    "    h_sig = np.sum(h_sig)\n",
    "    \n",
    "    w_sig = np.ones_like(y_sig) * (h_bkg / h_sig)\n",
    "    \n",
    "    ax.hist(y_sig, bins=bins, alpha=0.5, label='signal', color='purple', edgecolor='purple', \n",
    "            linewidth=2, hatch='//', histtype='step',\n",
    "            weights=w_sig)\n",
    "    \n",
    "    # compute significance\n",
    "    sig_mask = np.squeeze(y == 1.0)\n",
    "    bkg_mask = np.squeeze(y == 0.0)\n",
    "\n",
    "    cuts = np.linspace(0.0, 1.0, num=bins)\n",
    "    ams = []\n",
    "    w = np.concatenate([w_sig, w_bkg], axis=0)\n",
    "    \n",
    "    bx = ax.twinx()\n",
    "    \n",
    "    s, _ = np.histogram(y_sig, bins=bins, weights=w_sig)\n",
    "    b, _ = np.histogram(y_bkg, bins=bins, weights=w_bkg)\n",
    "    \n",
    "    for i in range(s.shape[0]):\n",
    "        s_i = np.sum(s[i:])\n",
    "        b_i = np.sum(b[i:])\n",
    "        \n",
    "        ams.append(s_i / np.sqrt(s_i + b_i))\n",
    "\n",
    "    k = np.argmax(ams)\n",
    "    \n",
    "    # add stuff to plot\n",
    "    bx.grid(False)\n",
    "    bx.plot(cuts, ams, color='g', label='Significance')\n",
    "\n",
    "    ax.axvline(x=cuts[k], linestyle='--', linewidth=2, color='g',\n",
    "               label=f'{round(cuts[k], 3)}: {round(ams[k], 3)}')\n",
    "\n",
    "    bx.set_ylabel(r'Significance: $s/\\sqrt{s+b}$')\n",
    "    \n",
    "    leg = ax.get_legend()\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.add_artist(leg)\n",
    "    \n",
    "    ax.set_xlabel('Class Label Probability')\n",
    "    ax.set_ylabel('Weighted Num. Events')\n",
    "    \n",
    "    # title\n",
    "    str0 = f'Category-{category} (#bins = {bins})'\n",
    "    str1 = f'@{(int(mass - delta), int(mass + delta))} dimuon_M (bkg)'\n",
    "    str2 = f'{title} @ {int(mass)}mA (signal {signal}), {str1}'\n",
    "    str3 = f'# signal = {sig.shape[0]}, # bkg = {bkg.shape[0]}'\n",
    "    \n",
    "    ax.set_title(f'{str0}\\n{str2}\\n{str3}')\n",
    "    \n",
    "    if isinstance(save, str):\n",
    "        path = utils.makedir(path)\n",
    "        plt.savefig(os.path.join(path, f'{save}.png'), bbox_inches='tight')\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutplot(model, dataset, category: int, signal: str, delta=50, bins=20, \n",
    "            size=(12, 10), legend='best', title='[pNN] Best Cut vs mA', seed=utils.SEED,\n",
    "            path='plot', save=None, show=True, ax=None):\n",
    "    \"\"\"Plots the value of the best cut as the mass (mA) varies\"\"\"\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=size)\n",
    "        ax = fig.gca()\n",
    "    \n",
    "    signal = dataset.signal\n",
    "    backgr = dataset.background\n",
    "    \n",
    "    cuts = []\n",
    "    \n",
    "    # select both signal and background in interval (m-d, m+d)\n",
    "    for mass in dataset.unique_signal_mass:\n",
    "        sig = signal[signal['mA'] == mass]\n",
    "        sig = sig[(sig['dimuon_M'] >= mass - delta) & (sig['dimuon_M'] < mass + delta)]\n",
    "\n",
    "        bkg = backgr\n",
    "        bkg = bkg[(bkg['dimuon_M'] >= mass - delta) & (bkg['dimuon_M'] < mass + delta)]\n",
    "\n",
    "        # prepare data\n",
    "        x = pd.concat([sig[dataset.columns['feature']],\n",
    "                       bkg[dataset.columns['feature']]], axis=0).values\n",
    "\n",
    "        y = np.reshape(pd.concat([sig['type'], bkg['type']], axis=0).values, newshape=[-1])\n",
    "        x = dict(x=x, m=np.ones_like(y[:, np.newaxis]) * mass)\n",
    "\n",
    "        # predict data\n",
    "        out = model.predict(x=x, batch_size=1024, verbose=0)\n",
    "        out = np.asarray(out)\n",
    "\n",
    "        y_sig = np.squeeze(out[y == 1.0])\n",
    "        y_bkg = np.squeeze(out[y == 0.0])\n",
    "    \n",
    "        # computing weights\n",
    "        w_bkg = bkg['weight'].values\n",
    "        w_sig = np.ones_like(y_sig)\n",
    "\n",
    "        h_bkg, _ = np.histogram(y_bkg, bins=bins, weights=w_bkg)\n",
    "        h_bkg = np.sum(h_bkg)\n",
    "\n",
    "        h_sig, _ = np.histogram(y_sig, bins=bins)\n",
    "        h_sig = np.sum(h_sig)\n",
    "\n",
    "        w_sig = np.ones_like(y_sig) * (h_bkg / h_sig)\n",
    "\n",
    "        # compute significance\n",
    "        ams = []\n",
    "\n",
    "        s, _ = np.histogram(y_sig, bins=bins, weights=w_sig)\n",
    "        b, _ = np.histogram(y_bkg, bins=bins, weights=w_bkg)\n",
    "\n",
    "        for i in range(s.shape[0]):\n",
    "            s_i = np.sum(s[i:])\n",
    "            b_i = np.sum(b[i:])\n",
    "\n",
    "            ams.append(s_i / np.sqrt(s_i + b_i))\n",
    "\n",
    "        k = np.argmax(ams)\n",
    "        cuts.append(np.linspace(0.0, 1.0, num=bins)[k])  # add cut value\n",
    "    \n",
    "    # plot\n",
    "    ax.plot(dataset.unique_signal_mass, cuts, marker='o', label=f'avg {round(np.mean(cuts), 2)}')\n",
    "    \n",
    "    ax.set_xlabel('Mass (GeV)')\n",
    "    ax.set_ylabel('Best Cut')\n",
    "    \n",
    "    ax.legend(loc=str(legend))\n",
    "    \n",
    "    # title\n",
    "    str0 = f'Category-{category} [#bins = {bins}]'\n",
    "    ax.set_title(f'{title} ({str0})')\n",
    "    \n",
    "    if isinstance(save, str):\n",
    "        path = utils.makedir(path)\n",
    "        plt.savefig(os.path.join(path, f'{save}.png'), bbox_inches='tight')\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_plot(model, dataset, mass: int, category: int, signal: str, delta=50, \n",
    "                     bins=20, size=(10, 9), legend='best', seed=utils.SEED, path='plot', save=None):\n",
    "    \"\"\"Plots the PR and ROC curves\"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "    \n",
    "    fig.set_figwidth(size[0] * 2)\n",
    "    fig.set_figheight(size[1])\n",
    "    \n",
    "    sig = dataset.signal[dataset.signal['mA'] == mass]\n",
    "    sig = sig[(sig['dimuon_M'] >= mass - delta) & (sig['dimuon_M'] < mass + delta)]\n",
    "    \n",
    "    bkg = dataset.background\n",
    "    bkg = bkg[(bkg['dimuon_M'] >= mass - delta) & (bkg['dimuon_M'] < mass + delta)]\n",
    "    \n",
    "    num_sig = sig.shape[0]\n",
    "    \n",
    "    # prepare data\n",
    "    x = pd.concat([sig[dataset.columns['feature']],\n",
    "                   bkg[dataset.columns['feature']]], axis=0).values\n",
    "\n",
    "    y = np.reshape(pd.concat([sig['type'], bkg['type']], axis=0).values, newshape=[-1])\n",
    "    x = dict(x=x, m=np.zeros_like(y[:, np.newaxis]) * mass)\n",
    "    \n",
    "    # predict data\n",
    "    out = model.predict(x=x, batch_size=1024, verbose=0)\n",
    "    out = np.asarray(out)\n",
    "    \n",
    "    y_sig = np.squeeze(out[y == 1.0])\n",
    "    y_bkg = np.squeeze(out[y == 0.0])\n",
    "    \n",
    "    # compute weights\n",
    "    w_bkg = bkg['weight'].values\n",
    "    \n",
    "    h_bkg, _ = np.histogram(y_bkg, bins=bins, weights=w_bkg)\n",
    "    h_bkg = np.sum(h_bkg)\n",
    "    \n",
    "    h_sig, _ = np.histogram(y_sig, bins=bins)\n",
    "    h_sig = np.sum(h_sig)\n",
    "    \n",
    "    w_sig = np.ones_like(y_sig) * (h_bkg / h_sig)\n",
    "    w = np.concatenate([w_sig, w_bkg], axis=0)\n",
    "    \n",
    "    str1 = f'@{(int(mass - delta), int(mass + delta))} dimuon_M (bkg)'\n",
    "    \n",
    "    # PR-curve\n",
    "    from sklearn.metrics import PrecisionRecallDisplay\n",
    "    \n",
    "    PrecisionRecallDisplay.from_predictions(y_true=y, y_pred=out, sample_weight=w, ax=axes[0],\n",
    "                                            name=f'pNN @ {int(mass)}mA (signal {signal}), {str1}')\n",
    "    axes[0].set_title(f'[pNN] PR Curve @ {int(mass)}mA (category {category})')\n",
    "    \n",
    "    # ROC curve\n",
    "    from sklearn.metrics import RocCurveDisplay\n",
    "    \n",
    "    RocCurveDisplay.from_predictions(y_true=y, y_pred=out, sample_weight=w, ax=axes[1],\n",
    "                                     name=f'pNN @ {int(mass)}mA (signal {signal}), {str1}')\n",
    "    axes[1].set_title(f'[pNN] ROC Curve @ {int(mass)}mA (category {category})')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if isinstance(save, str):\n",
    "        path = utils.makedir(path)\n",
    "        plt.savefig(os.path.join(path, f'{save}.png'), bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_CAT1 = [\"dimuon_deltar\", \"dimuon_deltaphi\", \"dimuon_deltaeta\", \"met_pt\", \n",
    "             \"deltar_bjet1_dimuon\", \"deltapt_bjet1_dimuon\", \"deltaeta_bjet1_dimuon\", \n",
    "             \"bjet_1_pt\", \"bjet_1_eta\", \"deltaphi_bjet1_dimuon\",\n",
    "             \"ljet_1_pt\", \"ljet_1_eta\", \"bjet_n\", \"ljet_n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset()\n",
    "data.load(signal='data/new/signal_bassociated_cat1.csv', \n",
    "          bkg='data/new/background_cat1.csv', feature_columns=VAR_CAT1)\n",
    "\n",
    "data.ds.loc[data.ds['ljet_1_eta'] == -10, 'ljet_1_eta'] = -3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: argument `case`!\n",
    "train, valid, test = get_data(data, features=VAR_CAT1, train_batch=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, checkpoint = utils.get_compiled_pnn(data, save='new/pnn-balanced-cat_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=train, epochs=100, validation_data=valid, verbose=2,\n",
    "          callbacks=[checkpoint, EarlyStopping(patience=40)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.load_from_checkpoint(model, path='new/pnn-balanced-cat_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.evaluate(x=test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mass in data.unique_signal_mass:\n",
    "    cmsplot(model, data, mass=mass, category=1, signal='bbH',\n",
    "            path='<YOUR-PATH>', save=f'significance_{int(mass)}mA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mass in data.unique_signal_mass:\n",
    "    performance_plot(model, data, mass=mass, category=1, signal='bbH',\n",
    "                     path='<YOUT-PATH>', save=f'curves_{int(mass)}mA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutplot(model, data, category=1, signal='bbH', legend='lower right',\n",
    "        path='<YOUT-PATH>', save='best-cut_vs_mA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
