{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TH1F, TH2F, TCanvas, TF1, TPad, gROOT, TGaxis, gStyle, TList, TPaveStats, TLatex, TLegend, gPad, kTRUE, TGraphErrors, TGraph, TMultiGraph\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import table \n",
    "import numpy as np\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.callbacks import History, EarlyStopping\n",
    "from tensorflow.keras.metrics import Recall, Precision, BinaryAccuracy, AUC, FalseNegatives, TrueNegatives, FalsePositives, TruePositives\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras import Model, Input\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "\n",
    "from CMS_graphics import CMS_lumi\n",
    "from CMS_graphics import tdrstyle\n",
    "\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "gROOT.SetBatch(kTRUE)\n",
    "tdrstyle.setTDRStyle()\n",
    "\n",
    "CMS_lumi.lumi_13TeV = \"%0.1f fb^{-1}\" % (35.8) # 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFolder = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#signal_resampled_df = pd.read_csv(outputFolder + \"signal_resample.csv\")\n",
    "signal_df = pd.read_csv(outputFolder + \"signal.csv\")\n",
    "background_df = pd.read_csv(outputFolder + \"background.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature list for pNN with mass as single feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"mA\",\n",
    "            \"dimuon_deltar\",\n",
    "            \"dimuon_deltaphi\",\n",
    "            \"dimuon_deltaeta\",\n",
    "            \"met_pt\",\n",
    "            \"deltar_bjet1_dimuon\",\n",
    "            \"deltapt_bjet1_dimuon\",\n",
    "            \"deltaeta_bjet1_dimuon\",\n",
    "            \"bjet_1_pt\",\n",
    "            \"bjet_1_eta\",\n",
    "            \"ljet_1_pt\",\n",
    "            \"ljet_1_eta\",\n",
    "            \"bjet_n\",\n",
    "            \"ljet_n\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature list for pNN with mass as linear combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"dimuon_deltar\",\n",
    "            \"dimuon_deltaphi\",\n",
    "            \"dimuon_deltaeta\",\n",
    "            \"met_pt\",\n",
    "            \"deltar_bjet1_dimuon\",\n",
    "            \"deltapt_bjet1_dimuon\",\n",
    "            \"deltaeta_bjet1_dimuon\",\n",
    "            \"bjet_1_pt\",\n",
    "            \"bjet_1_eta\",\n",
    "            \"ljet_1_pt\",\n",
    "            \"ljet_1_eta\",\n",
    "            \"bjet_n\",\n",
    "            \"ljet_n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig, bkg = len(signal_df), len(background_df)\n",
    "total = sig + bkg\n",
    "print('Total samples:\\n    Total: {}\\n    Signal: {} ({:.2f}% of total)\\n    Background: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, sig, 100 * sig / total, bkg, 100*bkg/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split samples into train/validation/test than merge signal and background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "strain_df, stest_df = train_test_split(signal_df, test_size=0.2)\n",
    "strain_df, sval_df = train_test_split(strain_df, test_size=0.1)\n",
    "btrain_df, btest_df = train_test_split(background_df, test_size=0.2)\n",
    "btrain_df, bval_df = train_test_split(btrain_df, test_size=0.1)\n",
    "\n",
    "# Merge signal and background for every set\n",
    "train_df = (strain_df.append(btrain_df, ignore_index = True)).sample(frac=1, random_state=200).reset_index(drop=True)\n",
    "val_df = (sval_df.append(bval_df, ignore_index = True)).sample(frac=1, random_state=200).reset_index(drop=True)\n",
    "test_df = (stest_df[1000:].append(btest_df[1000:], ignore_index = True)).sample(frac=1, random_state=200).reset_index(drop=True)\n",
    "plot_df = (stest_df[:1000].append(btest_df[:1000], ignore_index = True)).sample(frac=1, random_state=200).reset_index(drop=True)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_a_labels = np.array(train_df['type']).reshape(-1, 1)\n",
    "val_a_labels = np.array(val_df['type']).reshape(-1, 1)\n",
    "test_a_labels = np.array(test_df['type']).reshape(-1, 1)\n",
    "plot_a_labels = np.array(plot_df['type']).reshape(-1, 1)\n",
    "\n",
    "train_a_features = train_df[train_df.columns & features].to_numpy()\n",
    "val_a_features = val_df[val_df.columns & features].to_numpy()\n",
    "test_a_features = test_df[test_df.columns & features].to_numpy()\n",
    "plot_a_features = plot_df[plot_df.columns & features].to_numpy()\n",
    "\n",
    "train_a_mass = np.array(train_df['mA']).reshape(-1, 1)\n",
    "val_a_mass = np.array(val_df['mA']).reshape(-1, 1)\n",
    "test_a_mass = np.array(test_df['mA']).reshape(-1, 1)\n",
    "plot_a_mass = np.array(plot_df['mA']).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Extract weights in np\n",
    "w2 = train_df[['weight']].to_numpy()\n",
    "w2 = w2.reshape(-1, 1)\n",
    "\n",
    "w_test = test_df[['weight']].to_numpy()\n",
    "w_test = w_test.reshape(-1,1)\n",
    "\n",
    "# Normalise data with StandardScaler from sklearn\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler_mass = StandardScaler()\n",
    "train_a_features = scaler.fit_transform(train_a_features)\n",
    "val_a_features = scaler.transform(val_a_features)\n",
    "test_a_features = scaler.transform(test_a_features)\n",
    "plot_a_features = scaler.transform(plot_a_features)\n",
    "\n",
    "train_a_mass = scaler_mass.fit_transform(train_a_mass)\n",
    "val_a_mass = scaler_mass.transform(val_a_mass)\n",
    "test_a_mass = scaler_mass.transform(test_a_mass)\n",
    "plot_a_mass = scaler_mass.transform(plot_a_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training labels shape:', train_a_labels.shape)\n",
    "print('Validation labels shape:', val_a_labels.shape)\n",
    "print('Test labels shape:', test_a_labels.shape)\n",
    "print('Plot labels shape:', plot_a_labels.shape)\n",
    "print('\\n')\n",
    "print('Training features shape:', train_a_features.shape)\n",
    "print('Validation features shape:', val_a_features.shape)\n",
    "print('Test features shape:', test_a_features.shape)\n",
    "print('Plot features shape:', plot_a_features.shape)\n",
    "print('\\n')\n",
    "print('Training mass shape:', train_a_mass.shape)\n",
    "print('Validation mass shape:', val_a_mass.shape)\n",
    "print('Test mass shape:', test_a_mass.shape)\n",
    "print('Plot mass shape:', plot_a_mass.shape)\n",
    "print('\\n')\n",
    "print('Training weights:', w2.shape)\n",
    "\n",
    "bkg_a, sig_a = np.bincount(train_df['type'])\n",
    "total_a = sig_a + bkg_a\n",
    "print('\\n')\n",
    "print('Training:\\n    Total: {}\\n    Signal: {} ({:.2f}% of total)\\n    Background: {} ({:.2f}% of total)\\n'.format(\n",
    "    total_a, sig_a, 100 * sig_a / total_a, bkg_a, 100*bkg_a/total_a))\n",
    "\n",
    "bkg2, sig2 = np.bincount(val_df['type'])\n",
    "total2 = sig2 + bkg2\n",
    "print('Validation:\\n    Total: {}\\n    Signal: {} ({:.2f}% of total)\\n    Background: {} ({:.2f}% of total)\\n'.format(\n",
    "    total2, sig2, 100 * sig2 / total2, bkg2, 100*bkg2/total2))\n",
    "\n",
    "bkg3, sig3 = np.bincount(test_df['type'])\n",
    "total3 = sig3 + bkg3\n",
    "print('Test:\\n    Total: {}\\n    Signal: {} ({:.2f}% of total)\\n    Background: {} ({:.2f}% of total)\\n'.format(\n",
    "    total3, sig3, 100 * sig3 / total3, bkg3, 100*bkg3/total3))\n",
    "\n",
    "bkg4, sig4 = np.bincount(plot_df['type'])\n",
    "total4 = sig4 + bkg4\n",
    "print('Plot:\\n    Total: {}\\n    Signal: {} ({:.2f}% of total)\\n    Background: {} ({:.2f}% of total)\\n'.format(\n",
    "    total4, sig4, 100 * sig4 / total4, bkg4, 100*bkg4/total4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0_a = (1 / bkg_a)*(total_a)/2.0 \n",
    "weight_for_1_a = (1 / sig_a)*(total_a)/2.0\n",
    "\n",
    "class_weight_a = {0: weight_for_0_a, 1: weight_for_1_a}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0_a))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bias_a = np.log([sig_a/bkg_a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EarlyStopping(monitor='val_loss', patience=10, min_delta = 0.0001, mode = 'min', restore_best_weights=True, verbose=1)\n",
    "METRICS = [\n",
    "      TruePositives(name='tp'),\n",
    "      FalsePositives(name='fp'),\n",
    "      TrueNegatives(name='tn'),\n",
    "      FalseNegatives(name='fn'), \n",
    "      BinaryAccuracy(name='accuracy'),\n",
    "      Precision(name='precision'),\n",
    "      Recall(name='recall'),\n",
    "      AUC(name='auc'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(metrics=METRICS, output_bias=None, n_nodes=[300,150,\n",
    "                                                           100,50], dropout=0.1, learning_rate=0.001):\n",
    "    if output_bias is not None:\n",
    "        output_bias = Constant(output_bias)\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Input(shape=(14,)))\n",
    "    for i, node in enumerate(n_nodes):\n",
    "        if i == 0:\n",
    "            model.add(Dense(node, activation = 'relu', kernel_initializer='random_normal'))\n",
    "            model.add(Dropout(dropout))\n",
    "        else:\n",
    "            model.add(Dense(node, activation = 'relu'))\n",
    "            model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation = 'sigmoid', bias_initializer=output_bias))\n",
    "\n",
    "    # Set loss function and optimizer algorithm \n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate), metrics = metrics)\n",
    "    #plot_model(model, to_file=\"model.png\", show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_functional(metrics=METRICS, output_bias=None, n_nodes=[300,150,\n",
    "                                                           100,50], dropout=0.1, learning_rate=0.001):\n",
    "    if output_bias is not None:\n",
    "        output_bias = Constant(output_bias)    \n",
    "    \n",
    "    mass = Input(shape=(1,), name='mass')\n",
    "    m = Dense(units=23, bias_initializer='glorot_uniform',\n",
    "                  kernel_initializer='glorot_normal')(mass)\n",
    "    features = Input(shape=(13,), name='features')\n",
    "    \n",
    "    x = concatenate([m, features])\n",
    "    \n",
    "    for i, node in enumerate(n_nodes):\n",
    "        if i == 0:\n",
    "            hidden = Dense(node, activation = 'relu', kernel_initializer='random_normal')(x)\n",
    "            hidden = Dropout(dropout)(hidden)\n",
    "        else:\n",
    "            hidden = Dense(node, activation = 'relu')(hidden)\n",
    "            hidden = Dropout(dropout)(hidden)\n",
    "    output = Dense(1, activation = 'sigmoid', bias_initializer=output_bias, name='Output')(hidden)\n",
    "\n",
    "    model = Model(inputs=[mass, features], outputs=output)\n",
    "\n",
    "    # Set loss function and optimizer algorithm \n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate), metrics = metrics)\n",
    "    #plot_model(model, to_file=\"model.png\", show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15000\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mass as single feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_weighted_model_a = make_model(output_bias=initial_bias_a)\n",
    "#print(signal_weighted_model_a.summary())\n",
    "plot_model(signal_weighted_model_a, to_file=\"model.png\", show_shapes=True)\n",
    "\n",
    "signal_weighted_a_history = signal_weighted_model_a.fit(train_a_features, \n",
    "          train_a_labels, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(val_a_features, val_a_labels),\n",
    "          class_weight=class_weight_a)\n",
    "          sample_weight=w2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_weighted_model_a.save('./saved_models/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mass preprocessed as a linear combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_weighted_model_a = make_model_functional(output_bias=initial_bias_a)\n",
    "#print(signal_weighted_model_a.summary())\n",
    "plot_model(signal_weighted_model_a, to_file=\"model.png\", show_shapes=True)\n",
    "\n",
    "signal_weighted_a_history = signal_weighted_model_a.fit({\"mass\":train_a_mass, \"features\":train_a_features}, \n",
    "          train_a_labels, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs,\n",
    "          callbacks=[callback],\n",
    "          validation_data=({\"mass\":val_a_mass, \"features\":val_a_features}, val_a_labels),\n",
    "          class_weight=class_weight_a,\n",
    "          sample_weight=w2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_weighted_model_a.save('./saved_models/model2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "def plot_metrics(history, filename='history_plot.png'):\n",
    "    metrics = ['loss', 'auc', 'precision', 'recall']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(2,2,n+1)\n",
    "        plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        if metric == 'loss':\n",
    "            plt.ylim([0, plt.ylim()[1]])\n",
    "        elif metric == 'auc':\n",
    "            plt.ylim([0.8,1])\n",
    "        else:\n",
    "            plt.ylim([0,1])\n",
    "\n",
    "        plt.legend()\n",
    "        plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(signal_weighted_a_history, filename=\"history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load here the model (model.h5 for mass as single feature, model2.h5 for mass as linear combination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_weighted_model_a = load_model('./saved_models/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mass as single feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_a_signal_weighted = signal_weighted_model_a.predict(train_a_features, batch_size=batch_size)\n",
    "test_predictions_a_signal_weighted = signal_weighted_model_a.predict(test_a_features, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mass as linear combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_a_signal_weighted = signal_weighted_model_a.predict({\"mass\": train_a_mass, \"features\":train_a_features}, batch_size=batch_size)\n",
    "test_predictions_a_signal_weighted = signal_weighted_model_a.predict({\"mass\": test_a_mass, \"features\":test_a_features}, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Output distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for output with 3 signal samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_distribution_3(canvas, y_pred_1, y_true_1, y_pred_2, y_true_2, y_pred_3, y_true_3):\n",
    "    sig = TH1F('signal_250', 'pNN output ; pNN output; (1/N) dN/dX;', 100, 0, 1)\n",
    "    bkg = TH1F('background_250', 'pNN Output; pNN output; (1/N) dN/dX;', 100, 0, 1)\n",
    "    sig2 = TH1F('signal_500', 'pNN output ; pNN output; (1/N) dN/dX;', 100, 0, 1)\n",
    "    bkg2 = TH1F('background_500', 'pNN Output; pNN output; (1/N) dN/dX;', 100, 0, 1)\n",
    "    sig3 = TH1F('signal_800', 'pNN output ; pNN output; (1/N) dN/dX;', 100, 0, 1)\n",
    "    bkg3 = TH1F('background_800', 'pNN Output; pNN output; (1/N) dN/dX;', 100, 0, 1)\n",
    "    i = np.arange(0.00, 1.00, 0.01)\n",
    "    j=0\n",
    "    for _, p in enumerate(i):\n",
    "        Ns=0\n",
    "        Nb=0\n",
    "        idx = np.where(y_pred_1 < p)[0]\n",
    "        for a in idx:\n",
    "            if y_true_1[a] == 1:\n",
    "                Ns=Ns+1\n",
    "        idx2 = np.where(y_pred_1 > p)[0]\n",
    "        for b in idx2:\n",
    "            if y_true_1[b] == 0:\n",
    "                Nb=Nb+1\n",
    "        sig.SetBinContent(j, Ns)\n",
    "        bkg.SetBinContent(j, Nb)\n",
    "        j=j+1\n",
    "        \n",
    "    k=0\n",
    "    for _, p in enumerate(i):\n",
    "        Ns2=0\n",
    "        Nb2=0\n",
    "        idx = np.where(y_pred_2 < p)[0]\n",
    "        for a in idx:\n",
    "            if y_true_2[a] == 1:\n",
    "                Ns2=Ns2+1\n",
    "        idx2 = np.where(y_pred_2 > p)[0]\n",
    "        for b in idx2:\n",
    "            if y_true_2[b] == 0:\n",
    "                Nb2=Nb2+1\n",
    "        sig2.SetBinContent(k, Ns2)\n",
    "        bkg2.SetBinContent(k, Nb2)\n",
    "        k=k+1\n",
    "        \n",
    "    z=0\n",
    "    for _, p in enumerate(i):\n",
    "        Ns3=0\n",
    "        Nb3=0\n",
    "        idx = np.where(y_pred_3 < p)[0]\n",
    "        for a in idx:\n",
    "            if y_true_3[a] == 1:\n",
    "                Ns3=Ns3+1\n",
    "        idx2 = np.where(y_pred_3 > p)[0]\n",
    "        for b in idx2:\n",
    "            if y_true_3[b] == 0:\n",
    "                Nb3=Nb3+1\n",
    "        sig3.SetBinContent(z, Ns3)\n",
    "        bkg3.SetBinContent(z, Nb3)\n",
    "        z=z+1\n",
    "    dx_sig = (sig.GetXaxis().GetXmax() - sig.GetXaxis().GetXmin())/sig.GetNbinsX()\n",
    "    dx_bkg = (bkg.GetXaxis().GetXmax() - bkg.GetXaxis().GetXmin())/bkg.GetNbinsX()\n",
    "    dx_sig2 = (sig2.GetXaxis().GetXmax() - sig2.GetXaxis().GetXmin())/sig2.GetNbinsX()\n",
    "    dx_bkg2 = (bkg2.GetXaxis().GetXmax() - bkg2.GetXaxis().GetXmin())/bkg2.GetNbinsX()\n",
    "    dx_sig3 = (sig3.GetXaxis().GetXmax() - sig3.GetXaxis().GetXmin())/sig3.GetNbinsX()\n",
    "    dx_bkg3 = (bkg3.GetXaxis().GetXmax() - bkg3.GetXaxis().GetXmin())/bkg3.GetNbinsX()\n",
    "    sig.Scale(1./sig.GetSumOfWeights()/dx_sig)\n",
    "    bkg.Scale(1./bkg.GetSumOfWeights()/dx_bkg)\n",
    "    sig2.Scale(1./sig2.GetSumOfWeights()/dx_sig2)\n",
    "    bkg2.Scale(1./bkg2.GetSumOfWeights()/dx_bkg2)\n",
    "    sig3.Scale(1./sig3.GetSumOfWeights()/dx_sig3)\n",
    "    bkg3.Scale(1./bkg3.GetSumOfWeights()/dx_bkg3)\n",
    "    sig.SetFillColorAlpha(4,0.1)\n",
    "    sig.SetLineColor(4)\n",
    "    sig.SetLineWidth(12)\n",
    "    sig.SetStats(0)\n",
    "    bkg.SetLineColor(2)\n",
    "    bkg.SetFillColor(2)\n",
    "    bkg.SetFillStyle(3352)\n",
    "    bkg.SetLineWidth(12)\n",
    "    sig2.SetFillColorAlpha(4,0.1)\n",
    "    sig2.SetLineColor(4)\n",
    "    sig2.SetLineWidth(8)\n",
    "    sig2.SetStats(0)\n",
    "    bkg2.SetLineColor(2)\n",
    "    bkg2.SetFillColor(2)\n",
    "    bkg2.SetFillStyle(3352)\n",
    "    bkg2.SetLineWidth(8)\n",
    "    sig3.SetFillColorAlpha(4,0.1)\n",
    "    sig3.SetLineColor(4)\n",
    "    sig3.SetLineWidth(4)\n",
    "    sig3.SetStats(0)\n",
    "    bkg3.SetLineColor(2)\n",
    "    bkg3.SetFillColor(2)\n",
    "    bkg3.SetFillStyle(3352)\n",
    "    bkg3.SetLineWidth(4)\n",
    "    bkg3.Draw(\"HIST\")\n",
    "    sig.Draw(\"SAME HIST\")\n",
    "    sig2.Draw(\"SAME HIST\")\n",
    "    sig3.Draw(\"SAME HIST\")\n",
    "    bkg2.Draw(\"SAME HIST\")\n",
    "    bkg.Draw(\"SAME HIST\")\n",
    "    return sig, bkg, sig2, bkg2, sig3, bkg3, canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for output with 1 signal samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_distribution_1(canvas, y_pred_1, y_true_1):\n",
    "    sig = TH1F('signal_250', 'pNN output ; pNN output; (1/N) dN/dX ;', 50, 0, 1)\n",
    "    bkg = TH1F('background_250', 'pNN Output; pNN output; (1/N) dN/dX', 50, 0, 1)\n",
    "\n",
    "    i = np.arange(0.00, 1.00, 0.02)\n",
    "    j=0\n",
    "    for _, p in enumerate(i):\n",
    "        Ns=0\n",
    "        Nb=0\n",
    "        idx = np.where(y_pred_1 < p)[0]\n",
    "        for a in idx:\n",
    "            if y_true_1[a] == 1:\n",
    "                Ns=Ns+1\n",
    "        idx2 = np.where(y_pred_1 > p)[0]\n",
    "        for b in idx2:\n",
    "            if y_true_1[b] == 0:\n",
    "                Nb=Nb+1\n",
    "        sig.SetBinContent(j, Ns)\n",
    "        bkg.SetBinContent(j, Nb)\n",
    "        j=j+1\n",
    "    dx_sig = (sig.GetXaxis().GetXmax() - sig.GetXaxis().GetXmin())/sig.GetNbinsX()\n",
    "    dx_bkg = (bkg.GetXaxis().GetXmax() - bkg.GetXaxis().GetXmin())/bkg.GetNbinsX()\n",
    "    sig.Scale(1./sig.GetSumOfWeights()/dx_sig)\n",
    "    bkg.Scale(1./bkg.GetSumOfWeights()/dx_bkg)\n",
    "\n",
    "    sig.SetFillColorAlpha(4,0.1)\n",
    "    sig.SetLineColor(4)\n",
    "    sig.SetLineWidth(8)\n",
    "    sig.SetStats(0)\n",
    "    bkg.SetLineColor(2)\n",
    "    bkg.SetFillColor(2)\n",
    "    bkg.SetFillStyle(3352)\n",
    "    bkg.SetLineWidth(8)\n",
    "    sig.Draw(\"HIST\")\n",
    "    bkg.Draw(\"SAME HIST\")\n",
    "    return sig, bkg, canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do output distribution for mass 250,500,800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_250 = pd.read_csv(outputFolder + \"signal/MA250.csv\")\n",
    "df_500 = pd.read_csv(outputFolder + \"signal/MA500.csv\")\n",
    "df_800 = pd.read_csv(outputFolder + \"signal/MA800.csv\")\n",
    "\n",
    "###\n",
    "output_df_250 = (btest_df[0:4137].append(df_250, ignore_index = True)).sample(frac=1, random_state=200).reset_index(drop=True)\n",
    "output_features_250 = output_df_250[output_df_250.columns & features].to_numpy()\n",
    "output_features_250 = scaler.transform(output_features_250)\n",
    "output_prediction_250 = signal_weighted_model_a.predict(output_features_250, batch_size = batch_size)\n",
    "output_labels_250 = output_df_250[\"type\"].to_numpy()\n",
    "###\n",
    "output_df_500 = (btest_df[0:13630].append(df_500, ignore_index = True)).sample(frac=1, random_state=200).reset_index(drop=True)\n",
    "output_features_500 = output_df_500[output_df_500.columns & features].to_numpy()\n",
    "output_features_500 = scaler.transform(output_features_500)\n",
    "output_prediction_500 = signal_weighted_model_a.predict(output_features_500, batch_size = batch_size)\n",
    "output_labels_500 = output_df_500[\"type\"].to_numpy()\n",
    "###\n",
    "output_df_800 = (btest_df[0:14128].append(df_800, ignore_index = True)).sample(frac=1, random_state=200).reset_index(drop=True)\n",
    "output_features_800 = output_df_800[output_df_800.columns & features].to_numpy()\n",
    "output_features_800 = scaler.transform(output_features_800)\n",
    "output_prediction_800 = signal_weighted_model_a.predict(output_features_800, batch_size = batch_size)\n",
    "output_labels_800 = output_df_800[\"type\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = TCanvas('c1','output distribution', 4000, 2000)\n",
    "\n",
    "sig, bkg, sig2, bkg2, sig3, bkg3, canvas = output_distribution_3(c1, output_prediction_250, output_labels_250, output_prediction_500, output_labels_500, output_prediction_800, output_labels_800)\n",
    "\n",
    "legend = TLegend(.20,.9,.40,.75)\n",
    "legend.AddEntry(sig, \"Signal 250\",\"L\")\n",
    "legend.AddEntry(bkg, \"Background 250\",\"L\")\n",
    "legend.AddEntry(sig2, \"Signal 500\",\"L\")\n",
    "legend.AddEntry(bkg2, \"Background 500\",\"L\")\n",
    "legend.AddEntry(sig3, \"Signal 800\",\"L\")\n",
    "legend.AddEntry(bkg3, \"Background 800\",\"L\")\n",
    "legend.SetFillStyle(0)\n",
    "legend.SetLineWidth(0)\n",
    "legend.Draw()\n",
    "\n",
    "CMS_lumi.CMS_lumi(c1,4,0)\n",
    "c1.Draw()\n",
    "#c1.SaveAs(\"output_800.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the output distribution only for total signal vs total background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = TCanvas('c1','output distribution', 4000, 2000)\n",
    "\n",
    "sig, bkg, c1 = output_distribution_1(c1, test_predictions_a_signal_weighted, test_a_labels)\n",
    "\n",
    "legend = TLegend(.20,.88,.40,.75)\n",
    "legend.AddEntry(sig, \"Signal\",\"L\")\n",
    "legend.AddEntry(bkg, \"Background\",\"L\")\n",
    "legend.SetFillStyle(0)\n",
    "legend.SetLineWidth(0)\n",
    "legend.Draw()\n",
    "\n",
    "CMS_lumi.CMS_lumi(c1,4,0)\n",
    "c1.Draw()\n",
    "#c1.SaveAs(\"output_800.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of AUC vs MA (mass as single feature):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masses = [110,120,130,140,150,160,170,180,190,200,225,250,275,300,350,400,450,500,600,700,800,900,1000]\n",
    "AUC_array = []\n",
    "events = []\n",
    "for mA in masses:\n",
    "    df = pd.read_csv(outputFolder + \"signal/MA\" + str(mA) + \".csv\")\n",
    "    btest_df_upper = btest_df[btest_df[\"dimuon_M\"] < (mA + 80.)]\n",
    "    btest_df_mA = btest_df_upper[btest_df_upper[\"dimuon_M\"] > (mA - 80.)]\n",
    "    if len(df)<len(btest_df_mA):\n",
    "        output_df = (btest_df_mA[0:len(df)].append(df, ignore_index = True)).sample(frac=1, random_state=200).reset_index(drop=True)\n",
    "    else:\n",
    "        output_df = (btest_df_mA.append(df[0:len(btest_df_mA)], ignore_index = True)).sample(frac=1, random_state=200).reset_index(drop=True)\n",
    "    w_output = output_df[\"weight\"].to_numpy()\n",
    "    w_output = w_output.reshape(-1, 1)\n",
    "    output_features = output_df[output_df.columns & features].to_numpy()\n",
    "    output_features = scaler.transform(output_features)\n",
    "    output_prediction = signal_weighted_model_a.predict(output_features, batch_size = batch_size)\n",
    "    output_labels = output_df[\"type\"].to_numpy()\n",
    "    \n",
    "    weighted_results = signal_weighted_model_a.evaluate(output_features, output_labels,\n",
    "                                  batch_size=batch_size, sample_weight=w_output, verbose=0)\n",
    "                                  \n",
    "    AUC_array.append(weighted_results[-1])\n",
    "    \n",
    "    print(\"For mass: \" + str(mA))\n",
    "    print(\"Signal is: \" + str(len(df[0:len(btest_df_mA)])) + \" and background is: \" + str(len(btest_df_mA[0:len(df)])) + \"\\n\")\n",
    "    events.append(len(output_df))\n",
    "\n",
    "plt.plot(masses, AUC_array,'.-', label='events')\n",
    "plt.xlabel('mA', fontsize=15)\n",
    "plt.ylabel('AUC', fontsize=15)\n",
    "plt.tick_params(axis='x', labelsize=15)\n",
    "plt.tick_params(axis='y', labelsize=15)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('AUCvsMass.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of AUC vs MA (mass as linear combination):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masses = [110,120,130,140,150,160,170,180,190,200,225,250,275,300,350,400,450,500,600,700,800,900,1000]\n",
    "AUC_array = []\n",
    "for mA in masses:\n",
    "    df = pd.read_csv(outputFolder + \"signal/MA\" + str(mA) + \".csv\")\n",
    "    btest_df_upper = btest_df[btest_df[\"dimuon_M\"] < (mA + 80.)]\n",
    "    btest_df_mA = btest_df_upper[btest_df_upper[\"dimuon_M\"] > (mA - 80.)]\n",
    "    output_df = (btest_df[0:len(df)].append(df, ignore_index = True)).sample(frac=1, random_state=200).reset_index(drop=True)\n",
    "    output_features = output_df[output_df.columns & features].to_numpy()\n",
    "    output_features = scaler.transform(output_features)\n",
    "    output_mass = np.array(output_df[\"mA\"]).reshape(-1, 1)\n",
    "    output_mass = scaler_mass.transform(output_mass)\n",
    "    output_prediction = signal_weighted_model_a.predict({\"mass\": output_mass, \"features\":output_features}, batch_size = batch_size)\n",
    "    output_labels = np.array(output_df[\"type\"]).reshape(-1,1)\n",
    "    \n",
    "    weighted_results = signal_weighted_model_a.evaluate({\"mass\": output_mass, \"features\":output_features}, output_labels,\n",
    "                                  batch_size=batch_size, verbose=0)\n",
    "                                  \n",
    "    AUC_array.append(weighted_results[-1])\n",
    "    \n",
    "    print(\"For mass: \" + str(mA))\n",
    "    print(\"Signal is: \" + str(len(df)) + \" and background is: \" + str(len(btest_df[0:len(df)])) + \"\\n\")\n",
    "    \n",
    "\n",
    "plt.plot(masses, AUC_array,'.-', label='AUC score')\n",
    "plt.xlabel('mA', fontsize=15)\n",
    "plt.ylabel('AUC', fontsize=15)\n",
    "plt.tick_params(axis='x', labelsize=15)\n",
    "plt.tick_params(axis='y', labelsize=15)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('AUCvsMass.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix (only for mass as single feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = confusion_matrix(labels, predictions > p)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    print('Background correctly identified (True Negatives): ', cm[0][0])\n",
    "    print('Background identified as signal (False Positives): ', cm[0][1])\n",
    "    print('Signal identified as background (False Negatives): ', cm[1][0])\n",
    "    print('Signal correctly identified (True Positives): ', cm[1][1])\n",
    "    print('Total signal: ', np.sum(cm[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_results = signal_weighted_model_a.evaluate(test_a_features, test_a_labels,\n",
    "                                  batch_size=batch_size, verbose=0)\n",
    "for name, value in zip(signal_weighted_model_a.metrics_names, weighted_results):\n",
    "    print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(test_a_labels, test_predictions_a_signal_weighted, p=0.47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC (only for mass as single feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, _ = roc_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(100*tp, 100*(1-fp), label=name + \": AUC={:.3f}\".format(roc_auc_score(labels, predictions)), linewidth=2, **kwargs)\n",
    "    plt.ylabel('Background rejection [%]', fontsize=20)\n",
    "    plt.xlabel('Signal efficiency [%]', fontsize=20)\n",
    "    plt.title('ROC curve', fontsize=20)\n",
    "    plt.tick_params(axis='x', labelsize=17)\n",
    "    plt.tick_params(axis='y', labelsize=17)\n",
    "    plt.xlim([-0.5, 100.5])\n",
    "    plt.ylim([-0.5, 100.5])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(\"Train pNN - Baseline\", train_a_labels, train_predictions_a_signal_weighted, color=colors[2])\n",
    "plot_roc(\"Test pNN - Baseline\", test_a_labels, test_predictions_a_signal_weighted, color=colors[2], linestyle='--')\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=17)\n",
    "plt.show()\n",
    "#plt.savefig(\"roc.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot significance curve (only for mass as single feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions_a_signal_weighted = signal_weighted_model_a.predict(plot_a_features, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficiency_curve(canvas, y_pred, y_true):\n",
    "    mg = TMultiGraph(\"Efficiency\",\"Efficiency\")\n",
    "    legend = TLegend(.18,.42,.38,.28)\n",
    "\n",
    "    pad1 = TPad(\"pad1\",\"\",0,0,1,1)\n",
    "    pad2 = TPad(\"pad2\",\"\",0,0,1,1)\n",
    "\n",
    "    pad2.SetFillStyle(4000) #will be transparent\n",
    "\n",
    "    pad1.Draw()\n",
    "    pad1.cd()\n",
    "\n",
    "    tot_sig = len(np.where(y_true == 1)[0])\n",
    "    tot_bkg = len(np.where(y_true == 0)[0])\n",
    "    \n",
    "    x=np.empty(0)\n",
    "    significance=np.empty(0)\n",
    "    eff_sig=np.empty(0)\n",
    "    eff_bkg=np.empty(0)\n",
    "    \n",
    "    i = np.arange(0.00, 1.00, 0.01)\n",
    "    j=0\n",
    "    for _, p in enumerate(i):\n",
    "        Ns=0\n",
    "        Nb=0\n",
    "        idx = np.where(y_pred > p)[0]\n",
    "        for a in idx:\n",
    "            if y_true[a] == 1:\n",
    "                Ns=Ns+1\n",
    "            elif y_true[a] == 0:\n",
    "                Nb=Nb+1\n",
    "        signif = Ns/np.sqrt(Ns+Nb)\n",
    "        cm = confusion_matrix(y_true, y_pred > p)\n",
    "        tpr = cm[1][1]/(cm[1][1] + cm[1][0])\n",
    "        fpr = 1-(cm[0][1]/(cm[0][1] + cm[0][0]))\n",
    "        eff_sig = np.append(eff_sig, tpr)\n",
    "        eff_bkg = np.append(eff_bkg, fpr)\n",
    "        x = np.append(x,p)\n",
    "        significance = np.append(significance, signif)\n",
    "        j=j+1\n",
    "    max_sig = np.amax(significance)\n",
    "    i, = np.where(np.isclose(significance, max_sig)) # floating-point\n",
    "    max_x = x[i]\n",
    "    ymin = 0\n",
    "    ymax = math.ceil(max_sig)\n",
    "    dy = (ymax-ymin)/0.8 \n",
    "    xmin = 0\n",
    "    xmax = 1\n",
    "    dx = 0.01 \n",
    "\n",
    "    graph1 = TGraph(j,np.array(x, dtype='d'),np.array(eff_sig, dtype='d'))\n",
    "    graph1.SetLineColor(4)\n",
    "    graph1.SetLineWidth(4)\n",
    "    graph1.Draw()\n",
    "    graph2 = TGraph(j,np.array(x, dtype='d'),np.array(eff_bkg, dtype='d'))\n",
    "    graph2.SetLineColor(2)\n",
    "    graph2.SetLineWidth(4)\n",
    "    graph2.SetTitle(\"Cut efficiencies and optimal cut\")\n",
    "    graph2.GetYaxis().SetTitle(\"Efficiency (%)\")\n",
    "    graph2.GetXaxis().SetLabelSize(0.04);\n",
    "    graph2.GetYaxis().SetLabelSize(0.04);\n",
    "    graph2.GetXaxis().SetTitle(\"pNN output\")\n",
    "    graph2.GetXaxis().SetTitleSize(0.04);\n",
    "    graph2.GetYaxis().SetTitleSize(0.04);\n",
    "    graph2.Draw()\n",
    "    mg.Add(graph2)\n",
    "    mg.Add(graph1)\n",
    "    graph2.GetXaxis().SetRangeUser(0, 1)\n",
    "    mg.Draw() \n",
    "    pad1.Modified()\n",
    "    pad1.SetGrid()\n",
    "    canvas.cd()\n",
    "\n",
    "    pad2.Range(xmin-0.206,ymin-0.163*dy,xmax+0.065,ymax+0.05*dy)\n",
    "    pad2.Draw()\n",
    "    pad2.cd()\n",
    "\n",
    "    graph = TGraph(j,np.array(x, dtype='d'),np.array(significance, dtype='d'))\n",
    "    graph.SetLineColor(3)\n",
    "    graph.SetLineWidth(8)\n",
    "    graph.Draw(\"same\")\n",
    "    pad2.Update()\n",
    "\n",
    "    CMS_lumi.CMS_lumi(pad2,4,0)\n",
    "\n",
    "    legend.AddEntry(graph1, \"Signal efficiency\",\"L\")\n",
    "    legend.AddEntry(graph2, \"Background rejection\",\"L\")\n",
    "    legend.AddEntry(graph, \"Significance S/#sqrt{S+B}\",\"L\")\n",
    "    legend.SetFillStyle(0)\n",
    "    legend.SetLineWidth(0)\n",
    "    legend.Draw()\n",
    "\n",
    "    a = TLatex(0.18,0.22,\"#splitline{Using \" + str(tot_sig) + \" signal and \" + str(tot_bkg) + \" background}{Maximum of significance is at \" + str(\"%.2f\" % max_x) + \" with a value of \" + str(\"%.2f\" % max_sig) + \"}\")\n",
    "    a.SetTextSize(0.031)\n",
    "    a.SetNDC(kTRUE)\n",
    "    a.Draw() \n",
    "    axis = TGaxis(xmax,ymin,xmax,ymax,ymin,ymax,515,\"L+\")\n",
    "    axis.SetLabelColor(3)\n",
    "    axis.SetLabelOffset(0.005)\n",
    "    axis.SetTitle(\"Significance\")\n",
    "    axis.SetTitleOffset(0.6)\n",
    "    axis.SetTextColor(3)\n",
    "    axis.Draw()\n",
    "\n",
    "    return canvas, mg, legend, axis, graph, a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = TCanvas('c2','significance', 4000, 2000)\n",
    "\n",
    "c2, efficiencies, legend, axis, significance, latex = efficiency_curve(c2, plot_predictions_a_signal_weighted, plot_a_labels)\n",
    "\n",
    "c2.Draw()\n",
    "c2.SaveAs(\"efficiencies.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
