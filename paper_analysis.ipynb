{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the paper \"Parametrized Neural Networks for HEP\" by Baldi at al. 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import optimizers as opt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from typing import List, Tuple, Dict, Union\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "SEED = None\n",
    "\n",
    "def set_random_seed(seed: int) -> int:\n",
    "    \"\"\"Sets the random seed for TensorFlow, numpy, python's random\"\"\"\n",
    "    global SEED\n",
    "    \n",
    "    if seed is not None:\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "        SEED = seed\n",
    "        print(f'Random seed {seed} set.')\n",
    "\n",
    "\n",
    "def tf_global_norm(tensors: list, **kwargs):\n",
    "    norms = [tf.norm(x, **kwargs) for x in tensors]\n",
    "    return tf.sqrt(tf.reduce_sum([norm * norm for norm in norms]))\n",
    "\n",
    "\n",
    "def free_mem():\n",
    "    return gc.collect()\n",
    "\n",
    "\n",
    "def dataset_from_tensors(tensors, batch_size: int, split=0.25, seed=SEED):\n",
    "    total_size = tensors[-1].shape[0]\n",
    "    val_size = int(total_size * split)\n",
    "    train_size = total_size - val_size\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(tensors)\n",
    "    dataset = dataset.shuffle(buffer_size=1024)\n",
    "\n",
    "    training_set = dataset.take(train_size)\n",
    "    training_set = training_set.batch(batch_size)\n",
    "    \n",
    "    validation_set = dataset.skip(train_size).take(val_size)\n",
    "    validation_set = validation_set.batch(batch_size)\n",
    "    \n",
    "    free_mem()\n",
    "    return training_set, validation_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 42 set.\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    TRAIN_PATH = os.path.join('data', 'paper', 'all_train.csv')\n",
    "    TEST_PATH = os.path.join('data', 'paper', 'all_test.csv')\n",
    "\n",
    "    def __init__(self, x_scaler=None, m_scaler=None):\n",
    "        self.ds = None\n",
    "        self.columns = []\n",
    "\n",
    "        self.x_scaler = x_scaler\n",
    "        self.m_scaler = m_scaler\n",
    "        \n",
    "        self.train_features = None\n",
    "        self.test_features = None\n",
    "        self.train_labels = None\n",
    "        self.test_labels = None\n",
    "        self.train_mass = None\n",
    "        self.test_mass = None\n",
    "        \n",
    "    def load(self, path: str, mass: Union[np.ndarray, tuple, list] = None, test_size=0.2, \n",
    "             fit_scaler=True, seed=SEED):\n",
    "        \"\"\"Loads the dataset:\n",
    "            - selects feature columns,\n",
    "            - scales the data if a sklearn.Scaler was provided,\n",
    "            - splits all the data into train and test sets,\n",
    "            - allows to select which mass to keep.\n",
    "        \"\"\"\n",
    "        if self.ds is not None:\n",
    "            return\n",
    "\n",
    "        print('loading...')\n",
    "        self.ds = pd.read_csv(path, dtype=np.float32, na_filter=False)\n",
    "        \n",
    "        # select columns\n",
    "        columns = dict(feature=self.ds.columns[1:-1],\n",
    "                       label=self.ds.columns[0], mass=self.ds.columns[-1])\n",
    "        self.columns = columns\n",
    "        \n",
    "        # drop some mass\n",
    "        if isinstance(mass, (list, tuple)):\n",
    "            print('selecting mass...')\n",
    "            self._select_mass(mass)\n",
    "            free_mem()\n",
    "\n",
    "        # select series\n",
    "        self.features = self.ds[columns['feature']]\n",
    "        self.labels = self.ds[columns['label']]\n",
    "        self.masses = self.ds[columns['mass']]\n",
    "        self.unique_mass = sorted(self.ds.mass.unique())\n",
    "        \n",
    "        # fit scaler\n",
    "        if fit_scaler and (self.x_scaler is not None):\n",
    "            self.x_scaler.fit(self.features.values)\n",
    "        \n",
    "        if fit_scaler and (self.m_scaler is not None):\n",
    "            self.m_scaler.fit(np.reshape(self.unique_mass, newshape=(-1, 1)))\n",
    "        \n",
    "        print('dataset loaded.')\n",
    "        free_mem()\n",
    "    \n",
    "    def get(self, mask=None) -> tuple:\n",
    "        if mask is not None:\n",
    "            features = self.features[mask].values\n",
    "            labels = self.labels[mask].values\n",
    "            mass = self.masses[mask].values\n",
    "        else:\n",
    "            features = self.features.values\n",
    "            labels = self.labels.values\n",
    "            mass = self.masses.values\n",
    "\n",
    "        mass = mass.reshape((-1, 1))\n",
    "        labels = labels.reshape((-1, 1))\n",
    "\n",
    "        if self.x_scaler is not None:\n",
    "            features = self.x_scaler.transform(features)\n",
    "        \n",
    "        if self.m_scaler is not None:\n",
    "            mass = self.m_scaler.transform(mass)\n",
    "\n",
    "        x = dict(x=features, m=mass)\n",
    "        y = labels\n",
    "        \n",
    "        free_mem()\n",
    "        return x, y\n",
    "    \n",
    "    def get_by_mass(self, mass: float) -> dict:\n",
    "        return self.get(mask=self.ds.mass == mass)\n",
    "    \n",
    "    def _select_mass(self, mass):\n",
    "        \"\"\"Selects only the given mass from the dataframe\"\"\"\n",
    "        for m in mass:\n",
    "            mask = self.ds.mass == m\n",
    "            self.ds.drop(index=self.ds[mask].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models definition\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "\"\"\"Dynamic step-dependent parameters: used as learning rate schedules\"\"\"\n",
    "\n",
    "from tensorflow.keras.optimizers import schedules\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "\n",
    "\n",
    "class DynamicParameter:\n",
    "    \"\"\"Interface for learning rate schedule wrappers as dynamic-parameters\"\"\"\n",
    "    def __init__(self):\n",
    "        self._value = tf.Variable(initial_value=0.0, trainable=False, dtype=tf.float32)\n",
    "        self.step = tf.Variable(initial_value=0, trainable=False, dtype=tf.int32)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self._value.value()\n",
    "\n",
    "    @property\n",
    "    def variable(self):\n",
    "        return self._value\n",
    "\n",
    "    @value.setter\n",
    "    def value(self, value):\n",
    "        self._value.assign(value)\n",
    "\n",
    "    @staticmethod\n",
    "    def create(value: Union[float, int, LearningRateSchedule], **kwargs):\n",
    "        \"\"\"Converts a floating or LearningRateSchedule `value` into a DynamicParameter object\"\"\"\n",
    "        if isinstance(value, (DynamicParameter, ScheduleWrapper)):\n",
    "            return value\n",
    "\n",
    "        if isinstance(value, (float, int)):\n",
    "            return ConstantParameter(value)\n",
    "\n",
    "        if isinstance(value, LearningRateSchedule):\n",
    "            return ScheduleWrapper(schedule=value, **kwargs)\n",
    "\n",
    "        raise ValueError(f'Parameter \"value\" should be not {type(value)}.')\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.value\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        self._value.assign_sub(other)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        self._value.assign_add(other)\n",
    "\n",
    "    def serialize(self) -> dict:\n",
    "        return dict(step=int(self.step.value()))\n",
    "\n",
    "    def on_step(self):\n",
    "        self.step.assign_add(delta=1)\n",
    "\n",
    "    def load(self, config: dict):\n",
    "        self.step.assign(value=config.get('step', 0))\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        return {}\n",
    "\n",
    "\n",
    "class ScheduleWrapper(LearningRateSchedule, DynamicParameter):\n",
    "    \"\"\"A wrapper for built-in tf.keras' learning rate schedules\"\"\"\n",
    "    def __init__(self, schedule: LearningRateSchedule, min_value=1e-7, max_value=None):\n",
    "        super().__init__()\n",
    "        self.schedule = schedule\n",
    "        self.min_value = tf.constant(min_value, dtype=tf.float32)\n",
    "\n",
    "        if isinstance(max_value, (float, int)):\n",
    "            self.max_value = tf.constant(max_value, dtype=tf.float32)\n",
    "        else:\n",
    "            self.max_value = None\n",
    "\n",
    "        self._value.assign(value=self.schedule.initial_learning_rate)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        self.value = tf.maximum(self.min_value, self.schedule.__call__(self.step))\n",
    "\n",
    "        if self.max_value:\n",
    "            self.value = tf.minimum(self.value, self.max_value)\n",
    "\n",
    "        return self.value\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        return self.schedule.get_config()\n",
    "\n",
    "\n",
    "class ConstantParameter(DynamicParameter):\n",
    "    \"\"\"A constant learning rate schedule that wraps a constant float learning rate value\"\"\"\n",
    "    def __init__(self, value: float):\n",
    "        super().__init__()\n",
    "        self.value = value\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.value\n",
    "\n",
    "    def serialize(self) -> dict:\n",
    "        return {}\n",
    "\n",
    "\n",
    "class ExponentialDecay(ScheduleWrapper):\n",
    "    def __init__(self, initial_value: float, steps: int, rate: float, staircase=False, min_value=0.0, max_value=None):\n",
    "        super().__init__(schedule=schedules.ExponentialDecay(initial_learning_rate=float(initial_value),\n",
    "                                                             decay_steps=int(steps), decay_rate=float(rate),\n",
    "                                                             staircase=bool(staircase)),\n",
    "                         min_value=min_value, max_value=max_value)\n",
    "\n",
    "\n",
    "class StepDecay(ScheduleWrapper):\n",
    "    def __init__(self, initial_value: float, steps: int, rate: float, min_value=1e-7, max_value=None):\n",
    "        super().__init__(schedule=schedules.ExponentialDecay(initial_learning_rate=float(initial_value),\n",
    "                                                             decay_steps=int(steps), decay_rate=float(rate),\n",
    "                                                             staircase=True),\n",
    "                         min_value=min_value, max_value=max_value)\n",
    "\n",
    "\n",
    "class LinearDecay(ScheduleWrapper):\n",
    "    def __init__(self, initial_value: float, end_value: float, steps: int, cycle=False):\n",
    "        super().__init__(schedule=schedules.PolynomialDecay(initial_learning_rate=float(initial_value),\n",
    "                                                            decay_steps=int(steps), end_learning_rate=float(end_value),\n",
    "                                                            power=1.0, cycle=bool(cycle)))\n",
    "\n",
    "\n",
    "class PolynomialDecay(ScheduleWrapper):\n",
    "    def __init__(self, initial_value: float, end_value: float, steps: int, power=1.0, cycle=False):\n",
    "        super().__init__(schedule=schedules.PolynomialDecay(initial_learning_rate=float(initial_value),\n",
    "                                                            decay_steps=int(steps), end_learning_rate=float(end_value),\n",
    "                                                            power=power, cycle=bool(cycle)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PNN(Model):\n",
    "    \"\"\"A Parametric Neural Network (PNN) model with various way to condition *only* the input layer\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shapes: dict, weight_decay=0.0, **kwargs):\n",
    "        name = kwargs.pop('name', 'ParametricNN')\n",
    "        \n",
    "        inputs, outputs = self.structure(input_shapes, **kwargs)\n",
    "        super().__init__(inputs, outputs, name=name)\n",
    "\n",
    "        self.lr = None\n",
    "        \n",
    "        self.weight_decay = tf.constant(weight_decay, dtype=tf.float32)\n",
    "        self.should_decay_weights = self.weight_decay > 0.0\n",
    "        \n",
    "    def compile(self, optimizer_class=opt.Adam, loss='binary_crossentropy', metrics=None, lr=0.001, **kwargs):\n",
    "        self.lr = DynamicParameter.create(value=lr)\n",
    "        optimizer = optimizer_class(learning_rate=self.lr, **kwargs)\n",
    "\n",
    "        super().compile(optimizer, loss, metrics)\n",
    "\n",
    "    def structure(self, shapes: dict, units=128, num_layers=2, activation='relu', conditioning='concat',\n",
    "                  dropout=0.0, **kwargs) -> tuple:\n",
    "        inspect = kwargs.pop('inspect', False)\n",
    "        apply_dropout = dropout > 0.0\n",
    "        \n",
    "        inputs = self.inputs_from_shapes(shapes)\n",
    "        \n",
    "        if conditioning == 'paper':\n",
    "            # input concatenation\n",
    "            x = concatenate(list(inputs.values()))\n",
    "            \n",
    "        elif conditioning == 'concat':\n",
    "            # input concatenation + linear combination\n",
    "            x = concatenate(list(inputs.values()))\n",
    "            x = Dense(units=units, activation='linear', name='linear', **kwargs)(x)\n",
    "\n",
    "        elif conditioning == 'scaling':\n",
    "            # linear combination (m) + element-wise multiplication (x)\n",
    "            x = inputs['x']\n",
    "            m = inputs['m']\n",
    "\n",
    "            scaling = Dense(units=x.shape[-1], activation='linear', \n",
    "                            name='scaling', **kwargs)(m)\n",
    "            x = multiply([x, scaling])\n",
    "\n",
    "        elif conditioning == 'sigmoid':\n",
    "            # sigmoidal-gating\n",
    "            x = inputs['x']\n",
    "            m = inputs['m']\n",
    "\n",
    "            scaling = Dense(units=x.shape[-1], activation='sigmoid', \n",
    "                            name='scaling', **kwargs)(m)\n",
    "            x = multiply([x, scaling])\n",
    "\n",
    "        elif conditioning == 'softmax':\n",
    "            # softmax-gating\n",
    "            x = inputs['x']\n",
    "            m = inputs['m']\n",
    "\n",
    "            scaling = Dense(units=x.shape[-1], activation='softmax', \n",
    "                            name='scaling', **kwargs)(m)\n",
    "            x = multiply([x, scaling])\n",
    "\n",
    "        elif conditioning == 'affine':\n",
    "            # apply an affine transformation y = scale(m) * x + bias(m)\n",
    "            x = inputs['x']\n",
    "            m = inputs['m']\n",
    "\n",
    "            scale = Dense(units=x.shape[-1], activation='linear', \n",
    "                            name='scale', **kwargs)(m)\n",
    "\n",
    "            bias =  Dense(units=x.shape[-1], activation='linear', \n",
    "                            name='bias', **kwargs)(m)\n",
    "\n",
    "            x = multiply([x, scale])\n",
    "            x = add([x, bias])\n",
    "        else:\n",
    "            x = inputs['x']\n",
    "\n",
    "        if conditioning != 'paper' and apply_dropout:\n",
    "            x = Dropout(rate=dropout)(x)\n",
    "            \n",
    "        for _ in range(num_layers):\n",
    "            x = Dense(units=units, activation=activation, **kwargs)(x)\n",
    "            \n",
    "            if apply_dropout:\n",
    "                x = Dropout(rate=dropout)(x)\n",
    "        \n",
    "        outputs = Dense(units=1, activation='sigmoid', **kwargs)(x)\n",
    "\n",
    "        if inspect:\n",
    "            return inputs, [outputs, x]\n",
    "\n",
    "        return inputs, outputs\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, batch):\n",
    "        if isinstance(batch, tuple) and len(batch) == 1:\n",
    "            batch = batch[0]\n",
    "        \n",
    "        if len(batch) == 3:\n",
    "            x, labels, sample_weight = batch\n",
    "        else:\n",
    "            x, labels = batch\n",
    "            sample_weight = tf.ones_like(labels)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            classes = self(x, training=True)\n",
    "            loss = self.compiled_loss(labels, classes, sample_weight=sample_weight)\n",
    "            \n",
    "            weight_norm, l2_loss = self.decay_weights()\n",
    "            total_loss = loss + l2_loss\n",
    "            \n",
    "        global_norm, lr = self.apply_gradients(tape, loss)\n",
    "        self.lr.on_step()\n",
    "\n",
    "        debug = self.update_metrics(labels, classes, sample_weight=sample_weight)\n",
    "        debug['loss'] = tf.reduce_mean(loss)\n",
    "        debug['lr'] = lr\n",
    "        debug['norm'] = global_norm\n",
    "        debug['weight-norm'] = weight_norm\n",
    "        debug['l2-loss'] = l2_loss\n",
    "        \n",
    "        return debug\n",
    "    \n",
    "    def apply_gradients(self, tape, loss):\n",
    "        variables = self.trainable_variables\n",
    "\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, variables))\n",
    "\n",
    "        return tf_global_norm(grads), self.lr.value\n",
    "    \n",
    "    def decay_weights(self) -> tuple:\n",
    "        weight_norm = tf_global_norm(self.trainable_variables)\n",
    "            \n",
    "        if self.should_decay_weights:\n",
    "            l2_loss = weight_norm * self.weight_decay\n",
    "        else:\n",
    "            weight_norm = tf.stop_gradient(weight_norm)\n",
    "            l2_loss = 0.0\n",
    "        \n",
    "        return weight_norm, l2_loss\n",
    "    \n",
    "    def update_metrics(self, true, predicted, sample_weight=None) -> dict:\n",
    "        self.compiled_metrics.update_state(true, predicted, sample_weight=sample_weight)\n",
    "\n",
    "        return {metric.name: metric.result() for metric in self.metrics}\n",
    "\n",
    "    @staticmethod\n",
    "    def inputs_from_shapes(shapes: Dict[str, tuple]) -> Dict[str, Input]:\n",
    "        return {name: Input(shape=shape, name=name) for name, shape in shapes.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineConditioning(Layer):\n",
    "    \"\"\"Affine transform-based conditioning layer\"\"\"\n",
    "\n",
    "    def __init__(self, name=None, **kwargs):\n",
    "        super().__init__(name=name)\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        self.dense_scale: Dense = None\n",
    "        self.dense_bias: Dense = None\n",
    "\n",
    "        self.multiply = Multiply()\n",
    "        self.add = Add()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        shape, _ = input_shape\n",
    "        self.dense_scale = Dense(units=shape[-1], activation='linear', **self.kwargs)\n",
    "        self.dense_bias = Dense(units=shape[-1], activation='linear', **self.kwargs)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        assert isinstance(inputs, (list, tuple))\n",
    "        assert len(inputs) == 2\n",
    "\n",
    "        # condition input `x` on `z`\n",
    "        x, z = inputs\n",
    "\n",
    "        scale = self.dense_scale(z)\n",
    "        bias = self.dense_bias(z)\n",
    "\n",
    "        # apply affine transformation, i.e. y = scale(z) * x + bias(z)\n",
    "        y = self.multiply([x, scale])\n",
    "        y = self.add([y, bias])\n",
    "        return y\n",
    "\n",
    "\n",
    "class AffinePNN(PNN):\n",
    "    \"\"\"A PNN that uses Affine conditioning for all layers, not only the Input layer\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, weight_decay=0.0, mass_weights=None, mass_scaler=None, \n",
    "                 mass_intervals: Union[np.ndarray, List[tuple]] = None, **kwargs):\n",
    "        super().__init__(*args, weight_decay=weight_decay, **kwargs)\n",
    "        \n",
    "        # mass weights       \n",
    "        if isinstance(mass_weights, (list, tuple, np.ndarray)):\n",
    "            assert len(mass_weights) == len(mass_intervals)\n",
    "            \n",
    "            intervals = np.asarray(mass_intervals)\n",
    "            assert_2d_array(intervals)\n",
    "            \n",
    "            self.mass_low = intervals[:, 0]\n",
    "            self.mass_high = intervals[:, 1]\n",
    "            \n",
    "            # scale bins as input mass is scaled\n",
    "            if mass_scaler is not None:\n",
    "                self.mass_low = mass_scaler.transform(np.reshape(self.mass_low, newshape=(-1, 1)))\n",
    "                self.mass_low = tf.squeeze(self.mass_low)\n",
    "                \n",
    "                self.mass_high = mass_scaler.transform(np.reshape(self.mass_high, newshape=(-1, 1)))\n",
    "                self.mass_high = tf.squeeze(self.mass_high)\n",
    "            \n",
    "            self.mass_low = tf.cast(self.mass_low, dtype=tf.float32)\n",
    "            self.mass_high = tf.cast(self.mass_high, dtype=tf.float32)\n",
    "            \n",
    "            self.mass_weights = tf.squeeze(mass_weights)\n",
    "        else:\n",
    "            self.mass_weights = None\n",
    "        \n",
    "    def structure(self, shapes: dict, activation='relu', dropout=0.0, **kwargs) -> tuple:\n",
    "        inputs = self.inputs_from_shapes(shapes)\n",
    "        \n",
    "        apply_dropout = dropout > 0.0\n",
    "        units = kwargs.pop('units')\n",
    "        \n",
    "        x = inputs['x']\n",
    "        m = inputs['m']\n",
    "        \n",
    "        for i, unit in enumerate(units):\n",
    "            x = Dense(units=unit, activation=activation, **kwargs)(x)\n",
    "            x = AffineConditioning(name=f'affine-{i}')([x, m])\n",
    "            \n",
    "            if apply_dropout:\n",
    "                x = Dropout(rate=dropout)(x)\n",
    "        \n",
    "        out = Dense(units=1, activation='sigmoid', name='classes', **kwargs)(x)\n",
    "        return inputs, out\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, batch):\n",
    "        if isinstance(batch, tuple) and len(batch) == 1:\n",
    "            batch = batch[0]\n",
    "\n",
    "        if len(batch) == 3:\n",
    "            x, labels, sample_weight = batch\n",
    "        else:\n",
    "            x, labels = batch\n",
    "        \n",
    "            sample_weight = self.get_mass_weights(features=x, labels=labels)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            classes = self(x, training=True)\n",
    "            \n",
    "            loss = self.compiled_loss(labels, classes, \n",
    "                                      sample_weight=sample_weight)\n",
    "            \n",
    "            weight_norm, l2_loss = self.decay_weights()\n",
    "            total_loss = loss + l2_loss\n",
    "        \n",
    "        global_norm, lr = self.apply_gradients(tape, total_loss)\n",
    "        self.lr.on_step()\n",
    "        \n",
    "        debug = self.update_metrics(labels, classes, sample_weight=sample_weight)\n",
    "        debug['total_loss'] = tf.reduce_mean(total_loss)\n",
    "        debug['loss'] = tf.reduce_mean(loss)\n",
    "        debug['lr'] = lr\n",
    "        debug['norm'] = global_norm\n",
    "        debug['weight-norm'] = weight_norm\n",
    "        debug['l2_loss'] = l2_loss\n",
    "    \n",
    "        return debug\n",
    "    \n",
    "    def get_mass_weights(self, features, labels):\n",
    "        if ('m' in features) and tf.is_tensor(self.mass_weights):\n",
    "            mass = tf.cast(features['m'], dtype=tf.float32)\n",
    "            \n",
    "            # find which mass falls in which bin\n",
    "            mask = (mass >= self.mass_low) & (mass < self.mass_high)\n",
    "            bins = tf.argmax(tf.cast(mask, dtype=tf.int32), axis=-1)\n",
    "\n",
    "            # index by bin and label\n",
    "            indices = tf.concat([bins[:, None], tf.cast(labels, dtype=tf.int64)], axis=-1)\n",
    "\n",
    "            # retrieve weights\n",
    "            mass_weights = tf.gather_nd(self.mass_weights, indices)\n",
    "            mass_weights = tf.cast(mass_weights, dtype=tf.float32)[:, None]\n",
    "        else:\n",
    "            mass_weights = tf.ones_like(labels)\n",
    "            \n",
    "        return mass_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MassValidationCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, data: tuple, batch_size: int, early_stop: Union[bool, int] = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "        self.dataset = self.dataset.batch(batch_size)\n",
    "        \n",
    "        self.auc = tf.keras.metrics.AUC()\n",
    "        \n",
    "        # early stopping based on AUC value\n",
    "        if isinstance(early_stop, int):\n",
    "            self.should_early_stop = True\n",
    "            self.patience = early_stop\n",
    "            \n",
    "            self.stopped_epoch = 0\n",
    "            self.best_auc = np.Inf\n",
    "            self.wait = 0\n",
    "            self.best_weights = None\n",
    "        else:\n",
    "            self.should_early_stop = False\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        if not self.should_early_stop:\n",
    "            return\n",
    "        \n",
    "        self.stopped_epoch = 0\n",
    "        self.best_auc = np.Inf\n",
    "        self.wait = 0\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for (x, labels) in self.dataset:\n",
    "            classes = self.model(x, training=False)\n",
    "            \n",
    "            self.auc.update_state(labels, classes)\n",
    "        \n",
    "        auc = self.auc.result()\n",
    "        logs['mass_auc'] = np.round(auc, 4)\n",
    "        \n",
    "        if not self.should_early_stop:\n",
    "            return\n",
    "        \n",
    "        if np.greater(auc, self.best_auc):\n",
    "            self.best_auc = auc\n",
    "            self.wait = 0\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            \n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                \n",
    "                # restore model weights\n",
    "                self.model.set_weights(self.best_weights)\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.should_early_stop and (self.stopped_epoch > 0):\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce Paper Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mass = [499.99997, 750.0, 1000.0, 1250.0, 1500.0]\n",
    "train_mass = [499.99997, 1000.0, 1500.0]\n",
    "val_mass = [750.0, 1250.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "selecting mass...\n",
      "dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = Dataset(x_scaler=MinMaxScaler(), m_scaler=MinMaxScaler())\n",
    "data.load(path=Dataset.TRAIN_PATH, mass=train_mass)\n",
    "\n",
    "free_mem()\n",
    "\n",
    "x_train, y_train = data.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ParametricNN\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "m (InputLayer)                  [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x (InputLayer)                  [(None, 27)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 28)           0           m[0][0]                          \n",
      "                                                                 x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 500)          14500       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 500)          250500      dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 500)          250500      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 500)          250500      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 500)          250500      dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            501         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,017,001\n",
      "Trainable params: 1,017,001\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "val_split = 0.25\n",
    "\n",
    "# define model architecture as in Baldi et al.\n",
    "weight_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=SEED)\n",
    "\n",
    "model = PNN(input_shapes=dict(m=(1,), x=(data.features.shape[-1],)), units=500, num_layers=5, \n",
    "            conditioning='paper', kernel_initializer=weight_init)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# define optimizer and decaying learning rate schedule\n",
    "decay_steps = int(y_train.shape[0] * (1.0 - val_split) / batch_size)\n",
    "\n",
    "lr_schedule = StepDecay(0.0627, steps=decay_steps, rate=0.89)  # 0.1, 0.0627\n",
    "\n",
    "model.compile(optimizer_class=opt.SGD, momentum=0.5,\n",
    "              metrics=['binary_accuracy', metrics.AUC()], lr=lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "selecting mass...\n",
      "dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "# load mass-validation data\n",
    "val_data = Dataset(x_scaler=MinMaxScaler(), m_scaler=MinMaxScaler())\n",
    "val_data.load(path=Dataset.TRAIN_PATH, mass=val_mass)\n",
    "\n",
    "free_mem()\n",
    "x_val, y_val = val_data.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21005/21007 [============================>.] - ETA: 0s - loss: 0.1588 - binary_accuracy: 0.9007 - auc: 0.9627 - lr: 0.0627 - norm: 0.4874 - weight-norm: 102.4775 - l2-loss: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "callback = MassValidationCallback(data=(x_val, y_val), \n",
    "                                  batch_size=batch_size, early_stop=3)\n",
    "\n",
    "h = model.fit(x=x_train, y=y_train, batch_size=batch_size,\n",
    "              validation_split=val_split, callbacks=[callback],\n",
    "              epochs=10)  # the authors train for 200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1caac827ac0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('my_weights/paper_analysis/weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('my_weights/paper_analysis/weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "* Compare AUC metric towards various mass [`auc_vs_mass`] with error bars.\n",
    "* Compare AUC vs mass when some (or all) features from $x$ are dropped [`auc_vs_mass_no_features`].\n",
    "* Generalization capability by interpolation between unknown (not trained on) mass.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = Dataset(x_scaler=data.x_scaler, m_scaler=data.m_scaler, fit_scaler=False)\n",
    "test_data.load(path=Dataset.TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_underflow(metric, control_metric) -> float:\n",
    "    if metric == 0.0 and control_metric >= 0.9:\n",
    "        return 1.0  # underflow occurred (metric would be wrongly 0.0)\n",
    "    \n",
    "    return metric  # no uderflow\n",
    "\n",
    "\n",
    "def split_dataset(dataset: Dataset, data: tuple, num_folds=10, seed=SEED):\n",
    "    \"\"\"Splits a given Dataframe into k disjoint folds\"\"\"\n",
    "    fold_size = data[1].shape[0] // num_folds\n",
    "    folds = []\n",
    "    \n",
    "    # first, construct a dataframe from data, where `data = dict(x, m), y`\n",
    "    df = pd.concat([\n",
    "        pd.DataFrame(data[0]['x'], columns=dataset.columns['feature']),\n",
    "        pd.DataFrame(data[0]['m'], columns=['mass']),\n",
    "        pd.DataFrame(data[1], columns=['label'])\n",
    "    ], axis=1)\n",
    "    \n",
    "    for _ in range(num_folds - 1):\n",
    "        fold = df.sample(fold_size, random_state=seed)\n",
    "        folds.append(fold)\n",
    "        \n",
    "        df.drop(fold.index, inplace=True)\n",
    "    \n",
    "    folds.append(df)\n",
    "    \n",
    "    # make each `fold` be structured like `data`, i.e. fold = tuple(dict(x, m), y)\n",
    "    for i, fold in enumerate(folds):       \n",
    "        fold_x = dict(x=fold[dataset.columns['feature']].values, \n",
    "                      m=fold['mass'].values)\n",
    "        fold_y = fold['label'].values\n",
    "    \n",
    "        folds[i] = (fold_x, fold_y)\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_with_error(model, dataset, auc_index: int, mass: Union[np.ndarray, list, tuple], \n",
    "                   figsize=(26, 20), num_folds=10, verbose=0):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    auc = {fold: [] for fold in range(num_folds)}\n",
    "    \n",
    "    for m in mass:\n",
    "        x, y = dataset.get_by_mass(m)\n",
    "        \n",
    "        folds = split_dataset(dataset, data=(x, y), num_folds=num_folds)\n",
    "        \n",
    "        for i, fold in enumerate(folds):\n",
    "            score = model.evaluate(x=fold[0], y=fold[1], batch_size=128, verbose=verbose)\n",
    "\n",
    "            auc_score = round(score[auc_index], 4)\n",
    "            auc[i].append(check_underflow(auc_score, score[1]))\n",
    "        \n",
    "        print(f'Mass: {m}')\n",
    "    \n",
    "    # compute average AUC (over folds)\n",
    "    avg_auc = []\n",
    "    \n",
    "    for i, _ in enumerate(mass):\n",
    "        score = 0.0\n",
    "        \n",
    "        for fold in range(num_folds):\n",
    "            score += auc[fold][i]\n",
    "        \n",
    "        avg_auc.append(round(score / num_folds, 4))\n",
    "    \n",
    "    plt.title(f'AUC vs Mass')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.xlabel('Mass')\n",
    "    \n",
    "    plt.plot(mass, avg_auc, label='avg')\n",
    "    plt.scatter(mass, avg_auc, s=50, color='b')\n",
    "    \n",
    "    for i in range(num_folds):\n",
    "        plt.scatter(mass, auc[i], s=30, color='r')\n",
    "    \n",
    "    plt.show()\n",
    "    return auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Measure AUC for various mass values:\n",
    "If network was trained on a subset of available mass, then this also test its capability to interpolate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_with_error(model, test_data, auc_index=2, mass=all_mass, num_folds=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check if network is fooled by a single mass value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_vs_no_mass(model, dataset, auc_index: int, mass: Union[np.ndarray, list, tuple], const=[0], \n",
    "                   figsize=(26, 20), verbose=1):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    auc = {m: [] for m in const}\n",
    "\n",
    "    for m in mass:\n",
    "        x, y = dataset.get_by_mass(m)\n",
    "        \n",
    "        for fake_m in const:\n",
    "            x['m'] = np.ones_like(x['m']) * fake_m\n",
    "        \n",
    "            score = model.evaluate(x=x, y=y, batch_size=128, verbose=verbose)\n",
    "            \n",
    "            auc_score = round(score[auc_index], 4)\n",
    "            auc[fake_m].append(check_underflow(auc_score, score[1]))\n",
    "    \n",
    "    plt.title(f'AUC vs Mass')\n",
    "    \n",
    "    for fake_m in const:\n",
    "        plt.plot(mass, auc[fake_m], label=f'm-{round(fake_m, 2)}')\n",
    "        plt.scatter(mass, auc[fake_m], s=50)\n",
    "        \n",
    "    plt.legend(loc='best')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.xlabel('Mass')\n",
    "    \n",
    "    plt.show()\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_mass = np.linspace(-100, 100, 9)\n",
    "fake_mass = test_data.m_scaler.transform(np.reshape(fake_mass, newshape=(-1, 1)))\n",
    "fake_mass = np.squeeze(fake_mass)\n",
    "\n",
    "auc_vs_no_mass(model, test_data, auc_index=2, mass=all_mass, const=fake_mass, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How much the PNN relies on the features `x`?\n",
    "\n",
    "Set some (or all) of them to zero and compute the AUC w.r.t. the mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_vs_mass_no_features(model, dataset, auc_index: int, mass: Union[np.ndarray, list, tuple], \n",
    "                            figsize=(26, 20), features={}, verbose=1):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    auc = {k: [] for k in features.keys()}\n",
    "    \n",
    "    for label, indexes in features.items():\n",
    "        print(f'Features: {label}, {indexes}')\n",
    "        \n",
    "        for m in mass:\n",
    "            x, y = dataset.get_by_mass(m)\n",
    "        \n",
    "            # mask features\n",
    "            for i in indexes:\n",
    "                zero_feature = np.zeros_like(x['x'][:, i])\n",
    "                x['x'][:, i] = zero_feature\n",
    "        \n",
    "            print(f'Mass: {m}')\n",
    "            score = model.evaluate(x=x, y=y, batch_size=128, verbose=verbose)\n",
    "            \n",
    "            auc_score = round(score[auc_index], 4)\n",
    "            auc[label].append(check_underflow(auc_score, score[1]))\n",
    "            \n",
    "    plt.title(f'AUC vs Mass')\n",
    "    \n",
    "    for label in features.keys():\n",
    "        plt.plot(mass, auc[label])\n",
    "        plt.scatter(mass, auc[label], s=50, label=label)\n",
    "    \n",
    "    plt.ylabel('AUC')\n",
    "    plt.xlabel('Mass')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "    'all': [],\n",
    "    'none': np.arange(0, 13),\n",
    "}\n",
    "\n",
    "# add an entry telling to drop only the i-th feature column\n",
    "for i in range(13):\n",
    "    features[f'no-{i}'] = [i]\n",
    "\n",
    "auc_vs_mass_no_features(model, test_data, auc_index=2, mass=all_mass, features=features, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
